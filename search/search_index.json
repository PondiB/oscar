{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction OSCAR is a framework to efficiently support on-premises FaaS (Functions as a Service)for general-purpose file-processing computing applications. It represents the porting to an on-premises scenario of the SCAR framework , which supports a High Throughput Computing Programming Model to create highly-parallel event-driven file-processing serverless applications that execute on customized runtime environments provided by Docker containers run on AWS Lambda. Goal Users upload files to a bucket and this automatically triggers the execution of parallel invocations to a function responsible for processing each file. Output files are delivered into an output bucket for the convenience of the user. Highly scalable HTTP-based endpoints can also be offered to expose a generic application. A user-provided shell script is executed inside the container run from the user-defined Docker image to achieve the right execution environment for the application. Components OSCAR runs on an elastic Kubernetes cluster that is deployed using: EC3 , an open-source tool to deploy compute clusters that can horizontally scale in terms of the number of nodes with multiple plugins. IM , an open-source virtual infrastructure provisioning tool for multi-Clouds. CLUES , an elasticity manager that horizontally scales in and out the number of nodes of the Kubernetes cluster according to the workload. The following components are deployed inside the Kubernetes cluster in order to support the OSCAR platform: - MinIO , a high-performance distributed object storage server that provides an API compatible with S3. - Knative , a Serverless framework to serve container-based applications for synchronous invocations (default Serverless Backend). - OpenFaaS , a FaaS platform that allows creating functions executed via HTTP requests. - OSCAR, the main application, responsible for the management of the services and the integration of the different components to support event-driven serverless computing for file processing. It includes a web-based GUI aimed at end users to facilitate interaction with OSCAR. As external storage providers, the following services can be used: External MinIO servers, which may be in clusters other than the platform. [Amazon S3 , AWS's object storage service that offers industry-leading scalability, data availability, security, and performance in the public Cloud. Onedata , the global data access solution for science used in the EGI Federated Cloud . Any storage provider that can be accessible through WebDAV protocol. An example of a storage provider supporting this protocol is dCache , a storage middleware system capable of managing the storage and exchange of large data quantities. Note : All of the mentioned storage providers can be used as output, but only MinIO can be used as input. An OSCAR cluster can be accessed via its REST API , the [web-based UI and the command-line interface provided by oscar-cli .","title":"Introduction"},{"location":"#introduction","text":"OSCAR is a framework to efficiently support on-premises FaaS (Functions as a Service)for general-purpose file-processing computing applications. It represents the porting to an on-premises scenario of the SCAR framework , which supports a High Throughput Computing Programming Model to create highly-parallel event-driven file-processing serverless applications that execute on customized runtime environments provided by Docker containers run on AWS Lambda.","title":"Introduction"},{"location":"#goal","text":"Users upload files to a bucket and this automatically triggers the execution of parallel invocations to a function responsible for processing each file. Output files are delivered into an output bucket for the convenience of the user. Highly scalable HTTP-based endpoints can also be offered to expose a generic application. A user-provided shell script is executed inside the container run from the user-defined Docker image to achieve the right execution environment for the application.","title":"Goal"},{"location":"#components","text":"OSCAR runs on an elastic Kubernetes cluster that is deployed using: EC3 , an open-source tool to deploy compute clusters that can horizontally scale in terms of the number of nodes with multiple plugins. IM , an open-source virtual infrastructure provisioning tool for multi-Clouds. CLUES , an elasticity manager that horizontally scales in and out the number of nodes of the Kubernetes cluster according to the workload. The following components are deployed inside the Kubernetes cluster in order to support the OSCAR platform: - MinIO , a high-performance distributed object storage server that provides an API compatible with S3. - Knative , a Serverless framework to serve container-based applications for synchronous invocations (default Serverless Backend). - OpenFaaS , a FaaS platform that allows creating functions executed via HTTP requests. - OSCAR, the main application, responsible for the management of the services and the integration of the different components to support event-driven serverless computing for file processing. It includes a web-based GUI aimed at end users to facilitate interaction with OSCAR. As external storage providers, the following services can be used: External MinIO servers, which may be in clusters other than the platform. [Amazon S3 , AWS's object storage service that offers industry-leading scalability, data availability, security, and performance in the public Cloud. Onedata , the global data access solution for science used in the EGI Federated Cloud . Any storage provider that can be accessible through WebDAV protocol. An example of a storage provider supporting this protocol is dCache , a storage middleware system capable of managing the storage and exchange of large data quantities. Note : All of the mentioned storage providers can be used as output, but only MinIO can be used as input. An OSCAR cluster can be accessed via its REST API , the [web-based UI and the command-line interface provided by oscar-cli .","title":"Components"},{"location":"about/","text":"About OSCAR has been developed by the Grid and High Performance Computing Group (GRyCAP) at the Instituto de Instrumentaci\u00f3n para Imagen Molecular (I3M) from the Universitat Polit\u00e8cnica de Val\u00e8ncia (UPV) . This development is partially funded by the EGI Strategic and Innovation Fund and it can be deployed in the EGI Platform through the EGI Applications on Demand portal . Contact If you have any trouble please open an issue or email us .","title":"About"},{"location":"about/#about","text":"OSCAR has been developed by the Grid and High Performance Computing Group (GRyCAP) at the Instituto de Instrumentaci\u00f3n para Imagen Molecular (I3M) from the Universitat Polit\u00e8cnica de Val\u00e8ncia (UPV) . This development is partially funded by the EGI Strategic and Innovation Fund and it can be deployed in the EGI Platform through the EGI Applications on Demand portal .","title":"About"},{"location":"about/#contact","text":"If you have any trouble please open an issue or email us .","title":"Contact"},{"location":"api/","text":"OpenAPI Specification OSCAR exposes a secure REST API available at the Kubernetes master's node IP through an ingress. This API has been described following the OpenAPI Specification and it can be consulted bellow. SwaggerUIBundle({ url: 'api.yaml', dom_id: '#swagger-ui', })","title":"OpenAPI Specification"},{"location":"api/#openapi-specification","text":"OSCAR exposes a secure REST API available at the Kubernetes master's node IP through an ingress. This API has been described following the OpenAPI Specification and it can be consulted bellow. SwaggerUIBundle({ url: 'api.yaml', dom_id: '#swagger-ui', })","title":"OpenAPI Specification"},{"location":"deploy-ansible/","text":"Ansible playbook to deploy K3s and the OSCAR platform The folder deploy/ansible contains all the necessary files to deploy a K3s cluster together with the OSCAR platform using Ansible . This way, a minified Kubernetes distribution can be used to configure OSCAR on IoT devices located at the Edge, such as Raspberry PIs . Note that this playbook can also be applied to quickly spread the OSCAR platform on top of any machine or already started cloud instance since the playbook is compatible with GNU/Linux on ARM64 and AMD64 architectures. Requirements In order to use the playbook, you must install the following components: Ansible, following this guide . The netaddr python library. OpenSSH , to remotely access the hosts to be configured. Usage Clone the folder First of all, you must clone the OSCAR repo: git clone https://github.com/grycap/oscar.git And place into the ansible directory: cd oscar/deploy/ansible SSH configuration As Ansible is an agentless automation tool, you must configure the ~/.ssh/config file for granting access to the hosts to be configured via the SSH protocol. This playbook will use the Host field from SSH configuration to set the hostnames of the nodes, so please take care of naming them properly. Below you can find an example of a configuration file for four nodes, being the front the only one with a public IP, so it will be used as a proxy for the SSH connection to the working nodes ( ProxyJump option) via its internal network. Host front HostName <PUBLIC_IP> User ubuntu IdentityFile ~/.ssh/my_private_key Host wn1 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front Host wn2 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front Host wn3 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front Configuration of the inventory file Now, you have to edit the hosts file and add the hosts to be configured. Note that only one node must be set in the [front] section, while one or more nodes can be configured as working nodes of the cluster in the [wn] section. For example, for the previous SSH configuration the hosts inventory file should look like this: [front] ; Put here the frontend node as defined in .ssh/config (Host) front [wn] ; Put here the working nodes (one per line) as defined in the .ssh/config (Host) wn1 wn2 wn3 Setting up the playbook variables You also need to set up some parameters for the configuration of the cluster and OSCAR components, like OSCAR and MinIO credentials and DNS endpoints to configure the Kubernetes Ingress and cert-manager to securely expose the services. To do it, please edit the vars.yaml file and update the variables: --- # K3s version to be installed kube_version: v1.22.3+k3s1 # Token to login in K3s and the Kubernetes Dashboard kube_admin_token: kube-token123 # Password for OSCAR oscar_password: oscar123 # DNS name for the OSCAR Ingress and Kubernetes Dashboard (path \"/dashboard/\") dns_host: oscar-cluster.example.com # Password for MinIO minio_password: minio123 # DNS name for the MinIO API Ingress minio_dns_host: minio.oscar-cluster.example.com # DNS name for the MinIO Console Ingress minio_dns_host_console: minio-console.oscar-cluster.example.com Installation of the required ansible roles To install the required roles you only have to run: ansible-galaxy install -r install_roles.yaml --force The --force argument ensures you have the latest version of the roles. Running the playbook Finally, with the following command the ansible playbook will be executed, configuring the nodes set in the hosts inventory file: ansible-playbook -i hosts oscar-k3s.yaml","title":"Deployment on K3s with Ansible"},{"location":"deploy-ansible/#ansible-playbook-to-deploy-k3s-and-the-oscar-platform","text":"The folder deploy/ansible contains all the necessary files to deploy a K3s cluster together with the OSCAR platform using Ansible . This way, a minified Kubernetes distribution can be used to configure OSCAR on IoT devices located at the Edge, such as Raspberry PIs . Note that this playbook can also be applied to quickly spread the OSCAR platform on top of any machine or already started cloud instance since the playbook is compatible with GNU/Linux on ARM64 and AMD64 architectures.","title":"Ansible playbook to deploy K3s and the OSCAR platform"},{"location":"deploy-ansible/#requirements","text":"In order to use the playbook, you must install the following components: Ansible, following this guide . The netaddr python library. OpenSSH , to remotely access the hosts to be configured.","title":"Requirements"},{"location":"deploy-ansible/#usage","text":"","title":"Usage"},{"location":"deploy-ansible/#clone-the-folder","text":"First of all, you must clone the OSCAR repo: git clone https://github.com/grycap/oscar.git And place into the ansible directory: cd oscar/deploy/ansible","title":"Clone the folder"},{"location":"deploy-ansible/#ssh-configuration","text":"As Ansible is an agentless automation tool, you must configure the ~/.ssh/config file for granting access to the hosts to be configured via the SSH protocol. This playbook will use the Host field from SSH configuration to set the hostnames of the nodes, so please take care of naming them properly. Below you can find an example of a configuration file for four nodes, being the front the only one with a public IP, so it will be used as a proxy for the SSH connection to the working nodes ( ProxyJump option) via its internal network. Host front HostName <PUBLIC_IP> User ubuntu IdentityFile ~/.ssh/my_private_key Host wn1 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front Host wn2 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front Host wn3 HostName <PRIVATE_IP> User ubuntu IdentityFile ~/.ssh/my_private_key ProxyJump front","title":"SSH configuration"},{"location":"deploy-ansible/#configuration-of-the-inventory-file","text":"Now, you have to edit the hosts file and add the hosts to be configured. Note that only one node must be set in the [front] section, while one or more nodes can be configured as working nodes of the cluster in the [wn] section. For example, for the previous SSH configuration the hosts inventory file should look like this: [front] ; Put here the frontend node as defined in .ssh/config (Host) front [wn] ; Put here the working nodes (one per line) as defined in the .ssh/config (Host) wn1 wn2 wn3","title":"Configuration of the inventory file"},{"location":"deploy-ansible/#setting-up-the-playbook-variables","text":"You also need to set up some parameters for the configuration of the cluster and OSCAR components, like OSCAR and MinIO credentials and DNS endpoints to configure the Kubernetes Ingress and cert-manager to securely expose the services. To do it, please edit the vars.yaml file and update the variables: --- # K3s version to be installed kube_version: v1.22.3+k3s1 # Token to login in K3s and the Kubernetes Dashboard kube_admin_token: kube-token123 # Password for OSCAR oscar_password: oscar123 # DNS name for the OSCAR Ingress and Kubernetes Dashboard (path \"/dashboard/\") dns_host: oscar-cluster.example.com # Password for MinIO minio_password: minio123 # DNS name for the MinIO API Ingress minio_dns_host: minio.oscar-cluster.example.com # DNS name for the MinIO Console Ingress minio_dns_host_console: minio-console.oscar-cluster.example.com","title":"Setting up the playbook variables"},{"location":"deploy-ansible/#installation-of-the-required-ansible-roles","text":"To install the required roles you only have to run: ansible-galaxy install -r install_roles.yaml --force The --force argument ensures you have the latest version of the roles.","title":"Installation of the required ansible roles"},{"location":"deploy-ansible/#running-the-playbook","text":"Finally, with the following command the ansible playbook will be executed, configuring the nodes set in the hosts inventory file: ansible-playbook -i hosts oscar-k3s.yaml","title":"Running the playbook"},{"location":"deploy-ec3/","text":"Deployment with EC3 In order to deploy an elastic Kubernetes cluster with the OSCAR platform, it is preferable to use the IM Dashboard . Alternatively, you can also use EC3 , a tool that deploys elastic virtual clusters. EC3 uses the Infrastructure Manager (IM) to deploy such clusters on multiple Cloud back-ends. The installation details can be found here , though this section includes the relevant information to get you started. Prepare EC3 Clone the EC3 repository: git clone https://github.com/grycap/ec3 Download the OSCAR template into the ec3/templates folder: cd ec3 wget -P templates https://raw.githubusercontent.com/grycap/oscar/master/templates/oscar.radl Create an auth.txt authorization file with valid credentials to access your Cloud provider. As an example, to deploy on an OpenNebula-based Cloud site the contents of the file would be: type = OpenNebula; host = opennebula-host:2633; username = your-user; password = you-password Modify the corresponding RADL template in order to determine the appropriate configuration for your deployment: Virtual Machine Image identifiers Hardware Configuration As an example, to deploy in OpenNebula, one would modify the ubuntu-opennebula.radl (or create a new one). Deploy the cluster To deploy the cluster, execute: ./ec3 launch oscar-cluster oscar ubuntu-opennebula -a auth.txt This will take several minutes until the Kubernetes cluster and all the required services have been deployed. You will obtain the IP of the front-end of the cluster and a confirmation message that the front-end is ready. Notice that it will still take few minutes before the services in the Kubernetes cluster are up & running. Check the cluster state The cluster will be fully configured when all the Kubernetes pods are in the Running state. ./ec3 ssh oscar-cluster sudo kubectl get pods --all-namespaces Notice that initially only the front-end node of the cluster is deployed. As soon as the OSCAR framework is deployed, together with its services, the CLUES elasticity manager powers on a new (working) node on which these services will be run. You can see the status of the provisioned node(s) by issuing: clues status which obtains: | node |state| enabled |time stable|(cpu,mem) used |(cpu,mem) total| |-----------------|-----|---------|-----------|---------------|---------------| | wn1.localdomain | used| enabled | 00h00'49\" | 0.0,825229312 | 1,1992404992 | | wn2.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn3.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn4.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn5.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | The working nodes transition from off to powon and, finally, to the used status. Default Service Endpoints Once the OSCAR framework is running on the Kubernetes cluster, the endpoints described in the following table should be available. Most of the passwords/tokens are dynamically generated at deployment time and made available in the /var/tmp folder of the front-end node of the cluster. Service Endpoint Default User Password File OSCAR https://{FRONT_NODE} oscar oscar_password MinIO https://{FRONT_NODE}:30300 minio minio_secret_key OpenFaaS http://{FRONT_NODE}:31112 admin gw_password Kubernetes API https://{FRONT_NODE}:6443 tokenpass Kube. Dashboard https://{FRONT_NODE}:30443 dashboard_token Note that {FRONT_NODE} refers to the public IP of the front-end of the Kubernetes cluster. For example, to get the OSCAR password, you can execute: ./ec3 ssh oscar-cluster cat /var/tmp/oscar_password","title":"Deployment with EC3"},{"location":"deploy-ec3/#deployment-with-ec3","text":"In order to deploy an elastic Kubernetes cluster with the OSCAR platform, it is preferable to use the IM Dashboard . Alternatively, you can also use EC3 , a tool that deploys elastic virtual clusters. EC3 uses the Infrastructure Manager (IM) to deploy such clusters on multiple Cloud back-ends. The installation details can be found here , though this section includes the relevant information to get you started.","title":"Deployment with EC3"},{"location":"deploy-ec3/#prepare-ec3","text":"Clone the EC3 repository: git clone https://github.com/grycap/ec3 Download the OSCAR template into the ec3/templates folder: cd ec3 wget -P templates https://raw.githubusercontent.com/grycap/oscar/master/templates/oscar.radl Create an auth.txt authorization file with valid credentials to access your Cloud provider. As an example, to deploy on an OpenNebula-based Cloud site the contents of the file would be: type = OpenNebula; host = opennebula-host:2633; username = your-user; password = you-password Modify the corresponding RADL template in order to determine the appropriate configuration for your deployment: Virtual Machine Image identifiers Hardware Configuration As an example, to deploy in OpenNebula, one would modify the ubuntu-opennebula.radl (or create a new one).","title":"Prepare EC3"},{"location":"deploy-ec3/#deploy-the-cluster","text":"To deploy the cluster, execute: ./ec3 launch oscar-cluster oscar ubuntu-opennebula -a auth.txt This will take several minutes until the Kubernetes cluster and all the required services have been deployed. You will obtain the IP of the front-end of the cluster and a confirmation message that the front-end is ready. Notice that it will still take few minutes before the services in the Kubernetes cluster are up & running.","title":"Deploy the cluster"},{"location":"deploy-ec3/#check-the-cluster-state","text":"The cluster will be fully configured when all the Kubernetes pods are in the Running state. ./ec3 ssh oscar-cluster sudo kubectl get pods --all-namespaces Notice that initially only the front-end node of the cluster is deployed. As soon as the OSCAR framework is deployed, together with its services, the CLUES elasticity manager powers on a new (working) node on which these services will be run. You can see the status of the provisioned node(s) by issuing: clues status which obtains: | node |state| enabled |time stable|(cpu,mem) used |(cpu,mem) total| |-----------------|-----|---------|-----------|---------------|---------------| | wn1.localdomain | used| enabled | 00h00'49\" | 0.0,825229312 | 1,1992404992 | | wn2.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn3.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn4.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | | wn5.localdomain | off | enabled | 00h06'43\" | 0,0 | 1,1073741824 | The working nodes transition from off to powon and, finally, to the used status.","title":"Check the cluster state"},{"location":"deploy-ec3/#default-service-endpoints","text":"Once the OSCAR framework is running on the Kubernetes cluster, the endpoints described in the following table should be available. Most of the passwords/tokens are dynamically generated at deployment time and made available in the /var/tmp folder of the front-end node of the cluster. Service Endpoint Default User Password File OSCAR https://{FRONT_NODE} oscar oscar_password MinIO https://{FRONT_NODE}:30300 minio minio_secret_key OpenFaaS http://{FRONT_NODE}:31112 admin gw_password Kubernetes API https://{FRONT_NODE}:6443 tokenpass Kube. Dashboard https://{FRONT_NODE}:30443 dashboard_token Note that {FRONT_NODE} refers to the public IP of the front-end of the Kubernetes cluster. For example, to get the OSCAR password, you can execute: ./ec3 ssh oscar-cluster cat /var/tmp/oscar_password","title":"Default Service Endpoints"},{"location":"deploy-helm/","text":"Deployment on an existing Kubernetes cluster using Helm OSCAR can also be deployed on any existing Kubernetes cluster through its helm chart . However, to make the platform work properly, the following dependencies must be satisfied. A StorageClass with the ReadWriteMany access mode must be configured in the cluster for the creation of the persistent volume mounted on the service containers. For this purpose, we use the Kubernetes NFS-Client Provisioner , but there are other volume plugins that support this access mode. MinIO must be deployed and properly configured in the cluster. Its helm chart can be used for this purpose. It is important to configure it properly to have access from inside and outside the cluster, as the OSCAR's web interface connects directly to its API. In the OSCAR helm chart, you must indicate the values corresponding to its credentials and endpoint.","title":"Deployment with Helm"},{"location":"deploy-helm/#deployment-on-an-existing-kubernetes-cluster-using-helm","text":"OSCAR can also be deployed on any existing Kubernetes cluster through its helm chart . However, to make the platform work properly, the following dependencies must be satisfied. A StorageClass with the ReadWriteMany access mode must be configured in the cluster for the creation of the persistent volume mounted on the service containers. For this purpose, we use the Kubernetes NFS-Client Provisioner , but there are other volume plugins that support this access mode. MinIO must be deployed and properly configured in the cluster. Its helm chart can be used for this purpose. It is important to configure it properly to have access from inside and outside the cluster, as the OSCAR's web interface connects directly to its API. In the OSCAR helm chart, you must indicate the values corresponding to its credentials and endpoint.","title":"Deployment on an existing Kubernetes cluster using Helm"},{"location":"deploy-im-dashboard/","text":"Deployment with the IM Dashboard An OSCAR cluster can be easily deployed on multiple Cloud back-ends without requiring any installation by using the Infrastructure Manager 's Dashboard ( IM Dashboard ). This is a managed service provided by the GRyCAP research group at the Universitat Polit\u00e8cnica de Val\u00e8ncia to deploy customized virtual infrastructures across many Cloud providers. Using the IM Dashboard is the easiest and most convenient approach to deploy an OSCAR cluster. It also automatically allocates a DNS entry and TLS certificates to support HTTPS-based access to the OSCAR cluster and companion services (e.g. MinIO). This example shows how to deploy an OSCAR cluster on Amazon Web Services (AWS) with two nodes. Thanks to the IM, the very same procedure applies to deploy the OSCAR cluster in an on-premises Cloud (such as OpenStack) or any other Cloud provider supported by the IM. These are the steps: Access the IM Dashboard You will need to authenticate via EGI Check-In , which supports mutiple Identity Providers (IdP). Configure the Cloud Credentials Once logged in, you need to define the access credentials to the Cloud on which the OSCAR cluster will be deployed. These should be temporary credentials under the principle of least privilege (PoLP) . In our case we indicate an identifier for the set of credentials, the Access Key ID and the Secret Access Key for an IAM user that has privileges to deploy Virtual Machines in Amazon EC2 . Select the OSCAR template Customize and deploy the OSCAR cluster In this panel you can specify the number of Working Nodes (WNs) of the cluster together with the computational requirements for each node. We leave the default values. In this panel, specify the passwords to be employed to access the Kubernetes Web UI (Dashboard), to access the OSCAR web UI and to access the MinIO dashboard. These tokens can also be used for programmatic access to the respective services. Now, choose the Cloud provider. The ID specified when creating the Cloud credentials will be shown. You will also need to specify the Amazon Machine Image (AMI) identifier . We chose an AMI based on Ubuntu 20.04 provided by Canonical whose identifier for the us-east-1 region is: ami-09e67e426f25ce0d7 NOTE: You should obtain the AMI identifier for the latest version of the OS. This way, security patches will be already installed. You can obtain this AMI identifier from the AWS Marketplace or the Amazon EC2 service. Give the infrastructure a name and press \"Submit\". Check the status of the deployment OSCAR cluster You will see that the OSCAR cluster is being deployed and the infrastructure reaches the status \"running\". The process will not finish until it reaches the state \"configured\". If you are interested in understanding what is happening under the hood you can see the logs: Accessing the OSCAR cluster Once reached the \"configured\" state, see the \"Outputs\" to obtain the different endpoints: console_minio_endpoint: This endpoint brings access to the MinIO web user interface. dashboard_endpoint: This endpoint redirects to the Kubernetes dashboard where the OSCAR cluster is deployed. local_oscarui_endpoint: This endpoint is where the OSCAR backend is listening. It supports authentication only via basic-auth. minio_endpoint: Endpoint where the MinIO API is listening. If you access it through a web browser, you will be redirected to \"console_minio_endpoint\". oscarui_endpoint: Public endpoint of the OSCAR web user interface. It supports OIDC connections via EGI Check-in, as well as basic auth. The OSCAR UI can be accessed with the username oscar and the password you specified at deployment time. The MinIO UI can be accessed with the username minio and the password you specified at deployment time. The Kubernetes Dashboard can be accessed with the token you specified at deployment time. You can obtain statistics about the Kubernetes cluster: Terminating the OSCAR cluster You can terminate the OSCAR cluster from the IM Dashboard:","title":"Deployment with IM Dashboard"},{"location":"deploy-im-dashboard/#deployment-with-the-im-dashboard","text":"An OSCAR cluster can be easily deployed on multiple Cloud back-ends without requiring any installation by using the Infrastructure Manager 's Dashboard ( IM Dashboard ). This is a managed service provided by the GRyCAP research group at the Universitat Polit\u00e8cnica de Val\u00e8ncia to deploy customized virtual infrastructures across many Cloud providers. Using the IM Dashboard is the easiest and most convenient approach to deploy an OSCAR cluster. It also automatically allocates a DNS entry and TLS certificates to support HTTPS-based access to the OSCAR cluster and companion services (e.g. MinIO). This example shows how to deploy an OSCAR cluster on Amazon Web Services (AWS) with two nodes. Thanks to the IM, the very same procedure applies to deploy the OSCAR cluster in an on-premises Cloud (such as OpenStack) or any other Cloud provider supported by the IM. These are the steps: Access the IM Dashboard You will need to authenticate via EGI Check-In , which supports mutiple Identity Providers (IdP). Configure the Cloud Credentials Once logged in, you need to define the access credentials to the Cloud on which the OSCAR cluster will be deployed. These should be temporary credentials under the principle of least privilege (PoLP) . In our case we indicate an identifier for the set of credentials, the Access Key ID and the Secret Access Key for an IAM user that has privileges to deploy Virtual Machines in Amazon EC2 . Select the OSCAR template Customize and deploy the OSCAR cluster In this panel you can specify the number of Working Nodes (WNs) of the cluster together with the computational requirements for each node. We leave the default values. In this panel, specify the passwords to be employed to access the Kubernetes Web UI (Dashboard), to access the OSCAR web UI and to access the MinIO dashboard. These tokens can also be used for programmatic access to the respective services. Now, choose the Cloud provider. The ID specified when creating the Cloud credentials will be shown. You will also need to specify the Amazon Machine Image (AMI) identifier . We chose an AMI based on Ubuntu 20.04 provided by Canonical whose identifier for the us-east-1 region is: ami-09e67e426f25ce0d7 NOTE: You should obtain the AMI identifier for the latest version of the OS. This way, security patches will be already installed. You can obtain this AMI identifier from the AWS Marketplace or the Amazon EC2 service. Give the infrastructure a name and press \"Submit\". Check the status of the deployment OSCAR cluster You will see that the OSCAR cluster is being deployed and the infrastructure reaches the status \"running\". The process will not finish until it reaches the state \"configured\". If you are interested in understanding what is happening under the hood you can see the logs: Accessing the OSCAR cluster Once reached the \"configured\" state, see the \"Outputs\" to obtain the different endpoints: console_minio_endpoint: This endpoint brings access to the MinIO web user interface. dashboard_endpoint: This endpoint redirects to the Kubernetes dashboard where the OSCAR cluster is deployed. local_oscarui_endpoint: This endpoint is where the OSCAR backend is listening. It supports authentication only via basic-auth. minio_endpoint: Endpoint where the MinIO API is listening. If you access it through a web browser, you will be redirected to \"console_minio_endpoint\". oscarui_endpoint: Public endpoint of the OSCAR web user interface. It supports OIDC connections via EGI Check-in, as well as basic auth. The OSCAR UI can be accessed with the username oscar and the password you specified at deployment time. The MinIO UI can be accessed with the username minio and the password you specified at deployment time. The Kubernetes Dashboard can be accessed with the token you specified at deployment time. You can obtain statistics about the Kubernetes cluster: Terminating the OSCAR cluster You can terminate the OSCAR cluster from the IM Dashboard:","title":"Deployment with the IM Dashboard"},{"location":"egi-integration/","text":"Integration with the EGI Federated Cloud EGI is a federation of many cloud providers and hundreds of data centres, spread across Europe and worldwide that delivers advanced computing services to support scientists, multinational projects and research infrastructures. The EGI Federated Cloud is an IaaS-type cloud, made of academic private clouds and virtualised resources and built around open standards. Its development is driven by requirements of the scientific communities. EGI Applications on Demand: IM Dashboard The OSCAR platform can be deployed on the EGI Federated Cloud resources through the IM Dashboard available in the EGI Applications on Demand service. The IM Dashboard documentation can be followed in order to deploy the platform. EGI DataHub EGI DataHub , based on Onedata , provides a global data access solution for science. Integrated with the EGI AAI, it allows users to have Onedata spaces supported by providers across Europe for replicated storage and on-demand caching. EGI DataHub can be used as an output storage provider for OSCAR, allowing users to store the resulting files of their OSCAR services on a Onedata space. This can be done thanks to the FaaS Supervisor . Used in OSCAR and SCAR , responsible for managing the data Input/Output and the user code execution. To deploy a function with Onedata as output storage provider you only have to specify an identifier, the URL of the Oneprovider host, your access token and the name of your Onedata space in the \"Storage\" tab of the service creation wizard: And the path where you want to store the files in the \"OUTPUTS\" tab: This means that scientists can store their output files on their Onedata space in the EGI DataHub for long-time persistence and easy sharing of experimental results between researchers.","title":"Integration with EGI"},{"location":"egi-integration/#integration-with-the-egi-federated-cloud","text":"EGI is a federation of many cloud providers and hundreds of data centres, spread across Europe and worldwide that delivers advanced computing services to support scientists, multinational projects and research infrastructures. The EGI Federated Cloud is an IaaS-type cloud, made of academic private clouds and virtualised resources and built around open standards. Its development is driven by requirements of the scientific communities.","title":"Integration with the EGI Federated Cloud"},{"location":"egi-integration/#egi-applications-on-demand-im-dashboard","text":"The OSCAR platform can be deployed on the EGI Federated Cloud resources through the IM Dashboard available in the EGI Applications on Demand service. The IM Dashboard documentation can be followed in order to deploy the platform.","title":"EGI Applications on Demand: IM Dashboard"},{"location":"egi-integration/#egi-datahub","text":"EGI DataHub , based on Onedata , provides a global data access solution for science. Integrated with the EGI AAI, it allows users to have Onedata spaces supported by providers across Europe for replicated storage and on-demand caching. EGI DataHub can be used as an output storage provider for OSCAR, allowing users to store the resulting files of their OSCAR services on a Onedata space. This can be done thanks to the FaaS Supervisor . Used in OSCAR and SCAR , responsible for managing the data Input/Output and the user code execution. To deploy a function with Onedata as output storage provider you only have to specify an identifier, the URL of the Oneprovider host, your access token and the name of your Onedata space in the \"Storage\" tab of the service creation wizard: And the path where you want to store the files in the \"OUTPUTS\" tab: This means that scientists can store their output files on their Onedata space in the EGI DataHub for long-time persistence and easy sharing of experimental results between researchers.","title":"EGI DataHub"},{"location":"fdl-composer/","text":"Functions Definition Language Composer Writing an entire workflow in plain text could be a difficult task for many users. To simplify the process you can use FDL Composer , a web-based user interface to facilitate the definition of FDL YAML files for OSCAR and SCAR . How to access FDl Composer It does not require to be installed. Just access FDL Composer web . If you prefer to execute it on your computer instead of using the web, clone the git repository by using the following command: git clone https://github.com/grycap/fdl-composer And the run the app with npm : npm start Basic elements Workflows are composed of OSCAR services and Storage providers : OSCAR services OSCAR services are responsible for processing the data uploaded to Storage providers . Defining a new OSCAR service requires filling at least the name , image , and script fields. To define environment variables you must add them as a comma separated string of key=value entries. For example, to create a variable with the name firstName and the value John , the \"Environment variables\" field should look like firstName=John . If you want to assign more than one variable, for example, firstName and lastName with the values John and Keats , the input field should include them all separated by commas (e.g., firstName=John,lastName=Keats ). Storage providers and buckets/folders Storage providers are object storage systems responsible for storing both the input files to be processed by OSCAR services and the output files generated as a result of the processing. Three types of storage providers can be used in OSCAR FDLs : MinIO , Amazon S3 , and OneData . To configure them, drag the storage provider from the menu to the canvas and double click on the item created. A window with a single input will appear. Then, insert the path of the folder name. To edit one of the storage providers, move the mouse over the item and select the edit option. Remember that only MinIO can be used as input storage provider for OSCAR services. Download and load state The graphic workflow can be saved in a file using the \"Download state\" button. OSCAR services, Storage Providers, and Buckets are kept in the file. The graphic workflow can be edited later by loading it with the \"Load state\" button. Create a YAML file You can easily download the workflow's FDL file (in YAML) through the \"Export YAML\" button. Connecting components All components have four ports: The up and left ones are input ports while the right and down ports are used as output. OSCAR Services can only be connected with Storage providers , always linked in the same direction (the output of one element with the input of the other). When two services are connected, both will be declared in the FDL file, but they will work separately, and there will be no workflow between them. If two storage providers are connected between them, it will have no effect, but both storages will be declared. SCAR options FDL Composer can also create FDL files for SCAR . This allows to define workflows that can be executed on the Edge or in on-premises Clouds through OSCAR, and on the public Cloud (AWS Lambda and/or AWS Batch) through SCAR. Example There is an example of fdl-composer implementing the video-process use case in our blog .","title":"FDL Composer"},{"location":"fdl-composer/#functions-definition-language-composer","text":"Writing an entire workflow in plain text could be a difficult task for many users. To simplify the process you can use FDL Composer , a web-based user interface to facilitate the definition of FDL YAML files for OSCAR and SCAR .","title":"Functions Definition Language Composer"},{"location":"fdl-composer/#how-to-access-fdl-composer","text":"It does not require to be installed. Just access FDL Composer web . If you prefer to execute it on your computer instead of using the web, clone the git repository by using the following command: git clone https://github.com/grycap/fdl-composer And the run the app with npm : npm start","title":"How to access FDl Composer"},{"location":"fdl-composer/#basic-elements","text":"Workflows are composed of OSCAR services and Storage providers :","title":"Basic elements"},{"location":"fdl-composer/#oscar-services","text":"OSCAR services are responsible for processing the data uploaded to Storage providers . Defining a new OSCAR service requires filling at least the name , image , and script fields. To define environment variables you must add them as a comma separated string of key=value entries. For example, to create a variable with the name firstName and the value John , the \"Environment variables\" field should look like firstName=John . If you want to assign more than one variable, for example, firstName and lastName with the values John and Keats , the input field should include them all separated by commas (e.g., firstName=John,lastName=Keats ).","title":"OSCAR services"},{"location":"fdl-composer/#storage-providers-and-bucketsfolders","text":"Storage providers are object storage systems responsible for storing both the input files to be processed by OSCAR services and the output files generated as a result of the processing. Three types of storage providers can be used in OSCAR FDLs : MinIO , Amazon S3 , and OneData . To configure them, drag the storage provider from the menu to the canvas and double click on the item created. A window with a single input will appear. Then, insert the path of the folder name. To edit one of the storage providers, move the mouse over the item and select the edit option. Remember that only MinIO can be used as input storage provider for OSCAR services.","title":"Storage providers and buckets/folders"},{"location":"fdl-composer/#download-and-load-state","text":"The graphic workflow can be saved in a file using the \"Download state\" button. OSCAR services, Storage Providers, and Buckets are kept in the file. The graphic workflow can be edited later by loading it with the \"Load state\" button.","title":"Download and load state"},{"location":"fdl-composer/#create-a-yaml-file","text":"You can easily download the workflow's FDL file (in YAML) through the \"Export YAML\" button.","title":"Create a YAML file"},{"location":"fdl-composer/#connecting-components","text":"All components have four ports: The up and left ones are input ports while the right and down ports are used as output. OSCAR Services can only be connected with Storage providers , always linked in the same direction (the output of one element with the input of the other). When two services are connected, both will be declared in the FDL file, but they will work separately, and there will be no workflow between them. If two storage providers are connected between them, it will have no effect, but both storages will be declared.","title":"Connecting components"},{"location":"fdl-composer/#scar-options","text":"FDL Composer can also create FDL files for SCAR . This allows to define workflows that can be executed on the Edge or in on-premises Clouds through OSCAR, and on the public Cloud (AWS Lambda and/or AWS Batch) through SCAR.","title":"SCAR options"},{"location":"fdl-composer/#example","text":"There is an example of fdl-composer implementing the video-process use case in our blog .","title":"Example"},{"location":"fdl/","text":"Functions Definition Language (OSCAR) Example: functions: oscar: - oscar-test: name: plants memory: 2Gi cpu: '1.0' image: grycap/oscar-theano-plants script: plants.sh input: - storage_provider: minio.default path: example-workflow/in output: - storage_provider: minio.default path: example-workflow/med - oscar-test: name: grayify memory: 1Gi cpu: '1.0' image: grycap/imagemagick script: grayify.sh input: - storage_provider: minio.default path: example-workflow/med output: - storage_provider: minio.default path: example-workflow/res - storage_provider: onedata.my_onedata path: result-example-workflow - storage_provider: webdav.dcache path: example-workflow/res storage_providers: onedata: my_onedata: oneprovider_host: my_provider.com token: my_very_secret_token space: my_onedata_space webdav: dcache: hostname: my_dcache.com login: my_username password: my_password Top level parameters Field Description functions Functions Mandatory parameter to define a Functions Definition Language file. Note that \"functions\" instead of \"services\" has been used in order to keep compatibility with SCAR storage_providers StorageProviders Parameter to define the credentials for the storage providers to be used in the services clusters map[string] Cluster configuration for the OSCAR clusters that can be used as service's replicas, being the key the user-defined identifier for the cluster. Optional Functions Field Description oscar map[string] Service array Main object with the definition of the OSCAR services to be deployed. The components of the array are Service maps, where the key of every service is the identifier of the cluster where the service (defined as the value of the entry on the map) will be deployed. Service Field Description name string The name of the service cluster_id string Identifier for the current cluster, used to specify the cluster's StorageProvider in job delegations. OSCAR-CLI sets it using the ClusterID from the FDL. Optional. (default: \"\") image string Docker image for the service alpine boolean Alpine parameter to set if image is based on Alpine. If true a custom release of faas-supervisor will be used. Optional (default: false) script string Local path to the user script to be executed in the service container image_pull_secrets string array Array of Kubernetes secrets. Only needed to use private images located on private registries. memory string Memory limit for the service following the kubernetes format . Optional (default: 256Mi) cpu string CPU limit for the service following the kubernetes format . Optional (default: 0.2) enable_gpu bool Parameter to enable the use of GPU for the service. Requires a device plugin deployed on the cluster (More info: Kubernetes device plugins ). Optional (default: false) total_memory string Limit for the memory used by all the service's jobs running simultaneously. Apache YuniKorn scheduler is required to work. Same format as Memory, but internally translated to MB (integer). Optional (default: \"\") total_cpu string Limit for the virtual CPUs used by all the service's jobs running simultaneously. Apache YuniKorn scheduler is required to work. Same format as CPU, but internally translated to millicores (integer). Optional (default: \"\") synchronous SynchronousSettings Struct to configure specific sync parameters. This settings are only applied on Knative ServerlessBackend. Optional. replicas Replica array List of replicas to delegate jobs. Optional. rescheduler_threshold string Time (in seconds) that a job (with replicas) can be queued before delegating it. Optional. log_level string Log level for the FaaS Supervisor. Available levels: NOTSET, DEBUG, INFO, WARNING, ERROR and CRITICAL. Optional (default: INFO) input StorageIOConfig array Array with the input configuration for the service. Optional output StorageIOConfig array Array with the output configuration for the service. Optional environment EnvVarsMap The user-defined environment variables assigned to the service. Optional annotations map[string]string User-defined Kubernetes annotations to be set in job's definition. Optional labels map[string]string User-defined Kubernetes labels to be set in job's definition. Optional SynchronousSettings Field Description min_scale integer Minimum number of active replicas (pods) for the service. Optional. (default: 0) max_scale integer Maximum number of active replicas (pods) for the service. Optional. (default: 0 (Unlimited)) Replica Field Description type string Type of the replica to re-send events (can be oscar or endpoint ) cluster_id string Identifier of the cluster as defined in the \"clusters\" FDL field. Only used if Type is oscar service_name string Name of the service in the replica cluster. Only used if Type is oscar url string URL of the endpoint to re-send events (HTTP POST). Only used if Type is endpoint ssl_verify boolean Parameter to enable or disable the verification of SSL certificates. Only used if Type is endpoint . Optional. (default: true) priority integer Priority value to define delegation priority. Highest priority is defined as 0. If a delegation fails, OSCAR will try to delegate to another replica with lower priority. Optional. (default: 0) headers map[string]string Headers to send in delegation requests. Optional StorageIOConfig Field Description storage_provider string Reference to the storage provider defined in storage_providers . This string is composed by the provider's name (minio, s3, onedata) and identifier (defined by the user), separated by a point (e.g. \"minio.myidentifier\") path string Path in the storage provider. In MinIO and S3 the first directory of the specified path is translated into the bucket's name (e.g. \"bucket/folder/subfolder\") suffix string array Array of suffixes for filtering the files to be uploaded. Only used in the output field. Optional prefix string array Array of prefixes for filtering the files to be uploaded. Only used in the output field. Optional EnvVarsMap Field Description Variables map[string]string Map to define the environment variables that will be available in the service container StorageProviders Field Description minio map[string] MinIOProvider Map to define the credentials for a MinIO storage provider, being the key the user-defined identifier for the provider s3 map[string] S3Provider Map to define the credentials for a Amazon S3 storage provider, being the key the user-defined identifier for the provider onedata map[string] OnedataProvider Map to define the credentials for a Onedata storage provider, being the key the user-defined identifier for the provider webdav map[string] WebDavProvider Map to define the credentials for a storage provider accesible via WebDav protocol, being the key the user-defined identifier for the provider Cluster Field Description endpoint string Endpoint of the OSCAR cluster API auth_user string Username to connect to the cluster (basic auth) auth_password string Password to connect to the cluster (basic auth) ssl_verify boolean Parameter to enable or disable the verification of SSL certificates MinIOProvider Field Description endpoint string MinIO endpoint verify bool Verify MinIO's TLS certificates for HTTPS connections access_key string Access key of the MinIO server secret_key string Secret key of the MinIO server region string Region of the MinIO server S3Provider Field Description access_key string Access key of the AWS S3 service secret_key string Secret key of the AWS S3 service region string Region of the AWS S3 service OnedataProvider Field Description oneprovider_host string Endpoint of the Oneprovider token string Onedata access token space string Name of the Onedata space WebDAVProvider Field Description hostname string Provider hostname login string Provider account username password string Provider account password","title":"Functions Definition Language (FDL)"},{"location":"fdl/#functions-definition-language-oscar","text":"Example: functions: oscar: - oscar-test: name: plants memory: 2Gi cpu: '1.0' image: grycap/oscar-theano-plants script: plants.sh input: - storage_provider: minio.default path: example-workflow/in output: - storage_provider: minio.default path: example-workflow/med - oscar-test: name: grayify memory: 1Gi cpu: '1.0' image: grycap/imagemagick script: grayify.sh input: - storage_provider: minio.default path: example-workflow/med output: - storage_provider: minio.default path: example-workflow/res - storage_provider: onedata.my_onedata path: result-example-workflow - storage_provider: webdav.dcache path: example-workflow/res storage_providers: onedata: my_onedata: oneprovider_host: my_provider.com token: my_very_secret_token space: my_onedata_space webdav: dcache: hostname: my_dcache.com login: my_username password: my_password","title":"Functions Definition Language (OSCAR)"},{"location":"fdl/#top-level-parameters","text":"Field Description functions Functions Mandatory parameter to define a Functions Definition Language file. Note that \"functions\" instead of \"services\" has been used in order to keep compatibility with SCAR storage_providers StorageProviders Parameter to define the credentials for the storage providers to be used in the services clusters map[string] Cluster configuration for the OSCAR clusters that can be used as service's replicas, being the key the user-defined identifier for the cluster. Optional","title":"Top level parameters"},{"location":"fdl/#functions","text":"Field Description oscar map[string] Service array Main object with the definition of the OSCAR services to be deployed. The components of the array are Service maps, where the key of every service is the identifier of the cluster where the service (defined as the value of the entry on the map) will be deployed.","title":"Functions"},{"location":"fdl/#service","text":"Field Description name string The name of the service cluster_id string Identifier for the current cluster, used to specify the cluster's StorageProvider in job delegations. OSCAR-CLI sets it using the ClusterID from the FDL. Optional. (default: \"\") image string Docker image for the service alpine boolean Alpine parameter to set if image is based on Alpine. If true a custom release of faas-supervisor will be used. Optional (default: false) script string Local path to the user script to be executed in the service container image_pull_secrets string array Array of Kubernetes secrets. Only needed to use private images located on private registries. memory string Memory limit for the service following the kubernetes format . Optional (default: 256Mi) cpu string CPU limit for the service following the kubernetes format . Optional (default: 0.2) enable_gpu bool Parameter to enable the use of GPU for the service. Requires a device plugin deployed on the cluster (More info: Kubernetes device plugins ). Optional (default: false) total_memory string Limit for the memory used by all the service's jobs running simultaneously. Apache YuniKorn scheduler is required to work. Same format as Memory, but internally translated to MB (integer). Optional (default: \"\") total_cpu string Limit for the virtual CPUs used by all the service's jobs running simultaneously. Apache YuniKorn scheduler is required to work. Same format as CPU, but internally translated to millicores (integer). Optional (default: \"\") synchronous SynchronousSettings Struct to configure specific sync parameters. This settings are only applied on Knative ServerlessBackend. Optional. replicas Replica array List of replicas to delegate jobs. Optional. rescheduler_threshold string Time (in seconds) that a job (with replicas) can be queued before delegating it. Optional. log_level string Log level for the FaaS Supervisor. Available levels: NOTSET, DEBUG, INFO, WARNING, ERROR and CRITICAL. Optional (default: INFO) input StorageIOConfig array Array with the input configuration for the service. Optional output StorageIOConfig array Array with the output configuration for the service. Optional environment EnvVarsMap The user-defined environment variables assigned to the service. Optional annotations map[string]string User-defined Kubernetes annotations to be set in job's definition. Optional labels map[string]string User-defined Kubernetes labels to be set in job's definition. Optional","title":"Service"},{"location":"fdl/#synchronoussettings","text":"Field Description min_scale integer Minimum number of active replicas (pods) for the service. Optional. (default: 0) max_scale integer Maximum number of active replicas (pods) for the service. Optional. (default: 0 (Unlimited))","title":"SynchronousSettings"},{"location":"fdl/#replica","text":"Field Description type string Type of the replica to re-send events (can be oscar or endpoint ) cluster_id string Identifier of the cluster as defined in the \"clusters\" FDL field. Only used if Type is oscar service_name string Name of the service in the replica cluster. Only used if Type is oscar url string URL of the endpoint to re-send events (HTTP POST). Only used if Type is endpoint ssl_verify boolean Parameter to enable or disable the verification of SSL certificates. Only used if Type is endpoint . Optional. (default: true) priority integer Priority value to define delegation priority. Highest priority is defined as 0. If a delegation fails, OSCAR will try to delegate to another replica with lower priority. Optional. (default: 0) headers map[string]string Headers to send in delegation requests. Optional","title":"Replica"},{"location":"fdl/#storageioconfig","text":"Field Description storage_provider string Reference to the storage provider defined in storage_providers . This string is composed by the provider's name (minio, s3, onedata) and identifier (defined by the user), separated by a point (e.g. \"minio.myidentifier\") path string Path in the storage provider. In MinIO and S3 the first directory of the specified path is translated into the bucket's name (e.g. \"bucket/folder/subfolder\") suffix string array Array of suffixes for filtering the files to be uploaded. Only used in the output field. Optional prefix string array Array of prefixes for filtering the files to be uploaded. Only used in the output field. Optional","title":"StorageIOConfig"},{"location":"fdl/#envvarsmap","text":"Field Description Variables map[string]string Map to define the environment variables that will be available in the service container","title":"EnvVarsMap"},{"location":"fdl/#storageproviders","text":"Field Description minio map[string] MinIOProvider Map to define the credentials for a MinIO storage provider, being the key the user-defined identifier for the provider s3 map[string] S3Provider Map to define the credentials for a Amazon S3 storage provider, being the key the user-defined identifier for the provider onedata map[string] OnedataProvider Map to define the credentials for a Onedata storage provider, being the key the user-defined identifier for the provider webdav map[string] WebDavProvider Map to define the credentials for a storage provider accesible via WebDav protocol, being the key the user-defined identifier for the provider","title":"StorageProviders"},{"location":"fdl/#cluster","text":"Field Description endpoint string Endpoint of the OSCAR cluster API auth_user string Username to connect to the cluster (basic auth) auth_password string Password to connect to the cluster (basic auth) ssl_verify boolean Parameter to enable or disable the verification of SSL certificates","title":"Cluster"},{"location":"fdl/#minioprovider","text":"Field Description endpoint string MinIO endpoint verify bool Verify MinIO's TLS certificates for HTTPS connections access_key string Access key of the MinIO server secret_key string Secret key of the MinIO server region string Region of the MinIO server","title":"MinIOProvider"},{"location":"fdl/#s3provider","text":"Field Description access_key string Access key of the AWS S3 service secret_key string Secret key of the AWS S3 service region string Region of the AWS S3 service","title":"S3Provider"},{"location":"fdl/#onedataprovider","text":"Field Description oneprovider_host string Endpoint of the Oneprovider token string Onedata access token space string Name of the Onedata space","title":"OnedataProvider"},{"location":"fdl/#webdavprovider","text":"Field Description hostname string Provider hostname login string Provider account username password string Provider account password","title":"WebDAVProvider"},{"location":"invoking/","text":"Invoking services OSCAR services can be invoked synchronously and asynchronously sending an HTTP POST request to paths /run/<SERVICE_NAME> and /job/<SERVICE_NAME> respectively. For file processing, OSCAR automatically manages the creation and notification system of MinIO buckets in order to allow the event-driven invocation of services using asynchronous requests, generating a Kubernetes job for every file to be processed. Service access tokens As detailed in the API specification , invocation paths require the service access token in the request header for authentication. Service access tokens are auto-generated in service creation and update, and MinIO eventing system is automatically configured to use them for event-driven file processing. Tokens can be obtained through the API, using the oscar-cli service get command or directly from the web interface. Synchronous invocations Synchronous invocations allow obtaining the execution output as the response to the HTTP call to the /run/<SERVICE_NAME> path. For this, OSCAR delegates the execution to a Serverless Backend ( Knative or OpenFaaS ). Unlike asynchronous invocations, that are translated into Kubernetes jobs, synchronous invocations use a \"function\" pod to handle requests. This is possible thanks to the OpenFaaS Watchdog , which is injected into each service and is in charge of forking the process to be executed for each request received. Synchronous invocations can be made through OSCAR-CLI, using the comand oscar-cli service run : oscar-cli service run [SERVICE_NAME] {--input | --text-input} {-o | -output } You can check these use-cases: plant-classification-sync text-to-speech . The input can be sent as a file via the --input flag, and the result of the execution will be displayed directly in the terminal: oscar-cli service run plant-classification-sync --input images/image3.jpg Alternatively, it can be sent as plain text using the --text-input flag and the result stored in a file using the --output flag: oscar-cli service run text-to-speech --text-input \"Hello everyone\" --output output.mp3 Input/Output FaaS Supervisor , the component in charge of managing the input and output of services, allows JSON or base64 encoded body in service requests. The body of these requests will be automatically decoded into the invocation's input file available from the script through the $INPUT_FILE_PATH environment variable. The output of synchronous invocations will depend on the application itself: If the script generates a file inside the output dir available through the $TMP_OUTPUT_DIR environment variable, the result will be the file encoded in base64. If the script generates more than one file inside $TMP_OUTPUT_DIR , the result will be a zip archive containing all files encoded in base64. If there are no files in $TMP_OUTPUT_DIR , FaaS Supervisor will return its logs, including the stdout of the user script run. To avoid FaaS Supervisor's logs, you must set the service's log_level to CRITICAL . This way users can adapt OSCAR's services to their own needs. OSCAR-CLI OSCAR-CLI simplifies the execution of services synchronously via the oscar-cli service run command. This command requires the input to be passed as text through the --text-input flag or directly a file to be sent by passing its path through the --input flag. Both input types are automatically encoded in base64. It also allow setting the --output flag to indicate a path for storing (and decoding if needed) the output body in a file, otherwise the output will be shown in stdout. An illustration of triggering a service synchronously through OSCAR-CLI can be found in the cowsay example . cURL Naturally, OSCAR services can also be invoked via traditional HTTP clients such as cURL via the path /run/<SERVICE_NAME> . However, you must take care to properly format the input to one of the two supported formats (JSON or base64 encoded) and include the service access token in the request. An illustration of triggering a service synchronously through cURL can be found in the cowsay example . To send an input file through cURL, you must encode it in base64. To avoid issues with the output in synchronous invocations remember to put the log_level as CRITICAL . Output, which is encoded in base64, should be decoded as well. Save output in the expected format of the use-case. base64 input.png | curl -X POST -H \"Authorization: Bearer <TOKEN>\" \\ -d @- https://<CLUSTER_ENDPOINT>/run/<OSCAR_SERVICE> | base64 -d > result.png Limitations Using synchronous invocations couldn't be the best way to run long-running resource-demanding applications, like deep learning inference or video processing. This is due to the fact that Kubernetes Serverless Backends do not handle elasticity in the same way as their counterparts in public clouds, such as AWS Lambda. When multiple requests come to OpenFaaS, the function pod itself, with its resource specification (i.e. memory and CPU limits and requests), tries to process them simultaneously, which can cause an overload. Therefore, we consider Kubernetes job generation as the optimal approach to handle event-driven file processing through asynchronous invocations in OSCAR, being the execution of synchronous services a convenient way to support general lightweight container-based applications. Otherwise, OSCAR allows the configuration of the OpenFaaS Watchdog to limit the number of events to be processed by a function pod simultaneously. This can be done through the max_inflight option of the watchdog, which can be configured globally in the OSCAR deployment through the WATCHDOG_MAX_INFLIGHT environment variable and the helm chart parameter openfaas.watchdog.maxInflight .","title":"Invoking services"},{"location":"invoking/#invoking-services","text":"OSCAR services can be invoked synchronously and asynchronously sending an HTTP POST request to paths /run/<SERVICE_NAME> and /job/<SERVICE_NAME> respectively. For file processing, OSCAR automatically manages the creation and notification system of MinIO buckets in order to allow the event-driven invocation of services using asynchronous requests, generating a Kubernetes job for every file to be processed.","title":"Invoking services"},{"location":"invoking/#service-access-tokens","text":"As detailed in the API specification , invocation paths require the service access token in the request header for authentication. Service access tokens are auto-generated in service creation and update, and MinIO eventing system is automatically configured to use them for event-driven file processing. Tokens can be obtained through the API, using the oscar-cli service get command or directly from the web interface.","title":"Service access tokens"},{"location":"invoking/#synchronous-invocations","text":"Synchronous invocations allow obtaining the execution output as the response to the HTTP call to the /run/<SERVICE_NAME> path. For this, OSCAR delegates the execution to a Serverless Backend ( Knative or OpenFaaS ). Unlike asynchronous invocations, that are translated into Kubernetes jobs, synchronous invocations use a \"function\" pod to handle requests. This is possible thanks to the OpenFaaS Watchdog , which is injected into each service and is in charge of forking the process to be executed for each request received. Synchronous invocations can be made through OSCAR-CLI, using the comand oscar-cli service run : oscar-cli service run [SERVICE_NAME] {--input | --text-input} {-o | -output } You can check these use-cases: plant-classification-sync text-to-speech . The input can be sent as a file via the --input flag, and the result of the execution will be displayed directly in the terminal: oscar-cli service run plant-classification-sync --input images/image3.jpg Alternatively, it can be sent as plain text using the --text-input flag and the result stored in a file using the --output flag: oscar-cli service run text-to-speech --text-input \"Hello everyone\" --output output.mp3","title":"Synchronous invocations"},{"location":"invoking/#inputoutput","text":"FaaS Supervisor , the component in charge of managing the input and output of services, allows JSON or base64 encoded body in service requests. The body of these requests will be automatically decoded into the invocation's input file available from the script through the $INPUT_FILE_PATH environment variable. The output of synchronous invocations will depend on the application itself: If the script generates a file inside the output dir available through the $TMP_OUTPUT_DIR environment variable, the result will be the file encoded in base64. If the script generates more than one file inside $TMP_OUTPUT_DIR , the result will be a zip archive containing all files encoded in base64. If there are no files in $TMP_OUTPUT_DIR , FaaS Supervisor will return its logs, including the stdout of the user script run. To avoid FaaS Supervisor's logs, you must set the service's log_level to CRITICAL . This way users can adapt OSCAR's services to their own needs.","title":"Input/Output"},{"location":"invoking/#oscar-cli","text":"OSCAR-CLI simplifies the execution of services synchronously via the oscar-cli service run command. This command requires the input to be passed as text through the --text-input flag or directly a file to be sent by passing its path through the --input flag. Both input types are automatically encoded in base64. It also allow setting the --output flag to indicate a path for storing (and decoding if needed) the output body in a file, otherwise the output will be shown in stdout. An illustration of triggering a service synchronously through OSCAR-CLI can be found in the cowsay example .","title":"OSCAR-CLI"},{"location":"invoking/#curl","text":"Naturally, OSCAR services can also be invoked via traditional HTTP clients such as cURL via the path /run/<SERVICE_NAME> . However, you must take care to properly format the input to one of the two supported formats (JSON or base64 encoded) and include the service access token in the request. An illustration of triggering a service synchronously through cURL can be found in the cowsay example . To send an input file through cURL, you must encode it in base64. To avoid issues with the output in synchronous invocations remember to put the log_level as CRITICAL . Output, which is encoded in base64, should be decoded as well. Save output in the expected format of the use-case. base64 input.png | curl -X POST -H \"Authorization: Bearer <TOKEN>\" \\ -d @- https://<CLUSTER_ENDPOINT>/run/<OSCAR_SERVICE> | base64 -d > result.png","title":"cURL"},{"location":"invoking/#limitations","text":"Using synchronous invocations couldn't be the best way to run long-running resource-demanding applications, like deep learning inference or video processing. This is due to the fact that Kubernetes Serverless Backends do not handle elasticity in the same way as their counterparts in public clouds, such as AWS Lambda. When multiple requests come to OpenFaaS, the function pod itself, with its resource specification (i.e. memory and CPU limits and requests), tries to process them simultaneously, which can cause an overload. Therefore, we consider Kubernetes job generation as the optimal approach to handle event-driven file processing through asynchronous invocations in OSCAR, being the execution of synchronous services a convenient way to support general lightweight container-based applications. Otherwise, OSCAR allows the configuration of the OpenFaaS Watchdog to limit the number of events to be processed by a function pod simultaneously. This can be done through the max_inflight option of the watchdog, which can be configured globally in the OSCAR deployment through the WATCHDOG_MAX_INFLIGHT environment variable and the helm chart parameter openfaas.watchdog.maxInflight .","title":"Limitations"},{"location":"license/","text":"License Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2018 GRyCAP - I3M - UPV Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"license/#license","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2018 GRyCAP - I3M - UPV Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"local-testing/","text":"Local Testing with kind The easiest way to test the OSCAR platform locally is using kind . Kind allows the deployment of Kubernetes clusters inside Docker containers and automatically configures kubectl to access them. Prerequisites Docker , required by kind to launch the Kubernetes nodes on containers. Kubectl to communicate with the Kubernetes cluster. Helm to easily deploy applications on Kubernetes. Kind to deploy the local Kubernetes cluster. Automated local testing To set up the enviroment for the platform testing you can run the following command. This script automatically executes all the necessary steps to deploy the local cluster and the OSCAR platform along with all the required tools. curl -sSL http://go.oscar.grycap.net | bash Steps for manual local testing If you want to do it manualy you can follow the listed steps. Create the cluster To create a single node cluster with MinIO and Ingress controller ports locally accessible, run: cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 30300 hostPort: 30300 protocol: TCP - containerPort: 30301 hostPort: 30301 protocol: TCP EOF Deploy NGINX Ingress To enable Ingress support for accessing the OSCAR server, we must deploy the NGINX Ingress : kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml Deploy MinIO OSCAR depends on MinIO as a storage provider and function trigger. The easy way to run MinIO in a Kubernetes cluster is by installing its helm chart . To install the helm MinIO repo and install the chart, run the following commands replacing <MINIO_PASSWORD> with a password. It must have at least 8 characters: helm repo add minio https://charts.min.io helm install minio minio/minio --namespace minio --set rootUser=minio,\\ rootPassword=<MINIO_PASSWORD>,service.type=NodePort,service.nodePort=30300,\\ consoleService.type=NodePort,consoleService.nodePort=30301,mode=standalone,\\ resources.requests.memory=512Mi,\\ environment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 \\ --create-namespace Note that the deployment has been configured to use the rootUser minio and the specified password as rootPassword. The NodePort service type has been used in order to allow access from http://localhost:30300 (API) and http://localhost:30301 (Console). Deploy NFS server provisioner NFS server provisioner is required for the creation of ReadWriteMany PersistentVolumes in the kind cluster. This is needed by the OSCAR services to mount the volume with the FaaS Supervisor inside the job containers. To deploy it you can use this chart executing: helm repo add nfs-ganesha-server-and-external-provisioner https://kubernetes-sigs.github.io/nfs-ganesha-server-and-external-provisioner/ helm install nfs-server-provisioner nfs-ganesha-server-and-external-provisioner/nfs-server-provisioner Some Linux distributions may have problems using the NFS server provisioner with kind due to its default configuration of kernel-limit file descriptors. To workaround it, please run sudo sysctl -w fs.nr_open=1048576 . Deploy Knative Serving as Serverless Backend (OPTIONAL) OSCAR supports Knative Serving as Serverless Backend to process synchronous invocations . If you want to deploy it in the kind cluster, first you must deploy the Knative Operator kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.3.1/operator.yaml Note that the above command deploys the version v1.3.1 of the Operator. You can check if there are new versions here . Once the Operator has been successfully deployed, you can install the Knative Serving stack with the following command: cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1beta1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 1.3.0 ingress: kourier: enabled: true service-type: ClusterIP config: config-features: kubernetes.podspec-persistent-volume-claim: enabled kubernetes.podspec-persistent-volume-write: enabled network: ingress-class: \"kourier.ingress.networking.knative.dev\" EOF Deploy OSCAR First, create the oscar and oscar-svc namespaces by executing: kubectl apply -f https://raw.githubusercontent.com/grycap/oscar/master/deploy/yaml/oscar-namespaces.yaml Then, add the grycap helm repo and deploy by running the following commands replacing <OSCAR_PASSWORD> with a password of your choice and <MINIO_PASSWORD> with the MinIO rootPassword, and remember to add the flag --set serverlessBackend=knative if you deployed it in the previous step: helm repo add grycap https://grycap.github.io/helm-charts/ helm install --namespace=oscar oscar grycap/oscar \\ --set authPass=<OSCAR_PASSWORD> --set service.type=ClusterIP \\ --set ingress.create=true --set volume.storageClassName=nfs \\ --set minIO.endpoint=http://minio.minio:9000 --set minIO.TLSVerify=false \\ --set minIO.accessKey=minio --set minIO.secretKey=<MINIO_PASSWORD> Now you can access to the OSCAR web interface through https://localhost with user oscar and the specified password. Note that the OSCAR server has been configured to use the ClusterIP service of MinIO for internal communication. This blocks the MinIO section in the OSCAR web interface, so to download and upload files you must connect directly to MinIO ( http://localhost:30300 ). Delete the cluster Once you have finished testing the platform, you can remove the local kind cluster by executing: kind delete cluster Remember that if you have more than one cluster created, it may be required to set the --name flag to specify the name of the cluster to be deleted. Using OSCAR-CLI To use OSCAR-CLI in a local deployment, you should set the --disable-ssl flag to disable verification of the self-signed certificates: oscar-cli cluster add oscar-cluster https://localhost oscar <OSCAR_PASSWORD> --disable-ssl","title":"Local testing"},{"location":"local-testing/#local-testing-with-kind","text":"The easiest way to test the OSCAR platform locally is using kind . Kind allows the deployment of Kubernetes clusters inside Docker containers and automatically configures kubectl to access them.","title":"Local Testing with kind"},{"location":"local-testing/#prerequisites","text":"Docker , required by kind to launch the Kubernetes nodes on containers. Kubectl to communicate with the Kubernetes cluster. Helm to easily deploy applications on Kubernetes. Kind to deploy the local Kubernetes cluster.","title":"Prerequisites"},{"location":"local-testing/#automated-local-testing","text":"To set up the enviroment for the platform testing you can run the following command. This script automatically executes all the necessary steps to deploy the local cluster and the OSCAR platform along with all the required tools. curl -sSL http://go.oscar.grycap.net | bash","title":"Automated local testing"},{"location":"local-testing/#steps-for-manual-local-testing","text":"If you want to do it manualy you can follow the listed steps.","title":"Steps for manual local testing"},{"location":"local-testing/#create-the-cluster","text":"To create a single node cluster with MinIO and Ingress controller ports locally accessible, run: cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 30300 hostPort: 30300 protocol: TCP - containerPort: 30301 hostPort: 30301 protocol: TCP EOF","title":"Create the cluster"},{"location":"local-testing/#deploy-nginx-ingress","text":"To enable Ingress support for accessing the OSCAR server, we must deploy the NGINX Ingress : kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml","title":"Deploy NGINX Ingress"},{"location":"local-testing/#deploy-minio","text":"OSCAR depends on MinIO as a storage provider and function trigger. The easy way to run MinIO in a Kubernetes cluster is by installing its helm chart . To install the helm MinIO repo and install the chart, run the following commands replacing <MINIO_PASSWORD> with a password. It must have at least 8 characters: helm repo add minio https://charts.min.io helm install minio minio/minio --namespace minio --set rootUser=minio,\\ rootPassword=<MINIO_PASSWORD>,service.type=NodePort,service.nodePort=30300,\\ consoleService.type=NodePort,consoleService.nodePort=30301,mode=standalone,\\ resources.requests.memory=512Mi,\\ environment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 \\ --create-namespace Note that the deployment has been configured to use the rootUser minio and the specified password as rootPassword. The NodePort service type has been used in order to allow access from http://localhost:30300 (API) and http://localhost:30301 (Console).","title":"Deploy MinIO"},{"location":"local-testing/#deploy-nfs-server-provisioner","text":"NFS server provisioner is required for the creation of ReadWriteMany PersistentVolumes in the kind cluster. This is needed by the OSCAR services to mount the volume with the FaaS Supervisor inside the job containers. To deploy it you can use this chart executing: helm repo add nfs-ganesha-server-and-external-provisioner https://kubernetes-sigs.github.io/nfs-ganesha-server-and-external-provisioner/ helm install nfs-server-provisioner nfs-ganesha-server-and-external-provisioner/nfs-server-provisioner Some Linux distributions may have problems using the NFS server provisioner with kind due to its default configuration of kernel-limit file descriptors. To workaround it, please run sudo sysctl -w fs.nr_open=1048576 .","title":"Deploy NFS server provisioner"},{"location":"local-testing/#deploy-knative-serving-as-serverless-backend-optional","text":"OSCAR supports Knative Serving as Serverless Backend to process synchronous invocations . If you want to deploy it in the kind cluster, first you must deploy the Knative Operator kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.3.1/operator.yaml Note that the above command deploys the version v1.3.1 of the Operator. You can check if there are new versions here . Once the Operator has been successfully deployed, you can install the Knative Serving stack with the following command: cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1beta1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 1.3.0 ingress: kourier: enabled: true service-type: ClusterIP config: config-features: kubernetes.podspec-persistent-volume-claim: enabled kubernetes.podspec-persistent-volume-write: enabled network: ingress-class: \"kourier.ingress.networking.knative.dev\" EOF","title":"Deploy Knative Serving as Serverless Backend (OPTIONAL)"},{"location":"local-testing/#deploy-oscar","text":"First, create the oscar and oscar-svc namespaces by executing: kubectl apply -f https://raw.githubusercontent.com/grycap/oscar/master/deploy/yaml/oscar-namespaces.yaml Then, add the grycap helm repo and deploy by running the following commands replacing <OSCAR_PASSWORD> with a password of your choice and <MINIO_PASSWORD> with the MinIO rootPassword, and remember to add the flag --set serverlessBackend=knative if you deployed it in the previous step: helm repo add grycap https://grycap.github.io/helm-charts/ helm install --namespace=oscar oscar grycap/oscar \\ --set authPass=<OSCAR_PASSWORD> --set service.type=ClusterIP \\ --set ingress.create=true --set volume.storageClassName=nfs \\ --set minIO.endpoint=http://minio.minio:9000 --set minIO.TLSVerify=false \\ --set minIO.accessKey=minio --set minIO.secretKey=<MINIO_PASSWORD> Now you can access to the OSCAR web interface through https://localhost with user oscar and the specified password. Note that the OSCAR server has been configured to use the ClusterIP service of MinIO for internal communication. This blocks the MinIO section in the OSCAR web interface, so to download and upload files you must connect directly to MinIO ( http://localhost:30300 ).","title":"Deploy OSCAR"},{"location":"local-testing/#delete-the-cluster","text":"Once you have finished testing the platform, you can remove the local kind cluster by executing: kind delete cluster Remember that if you have more than one cluster created, it may be required to set the --name flag to specify the name of the cluster to be deleted.","title":"Delete the cluster"},{"location":"local-testing/#using-oscar-cli","text":"To use OSCAR-CLI in a local deployment, you should set the --disable-ssl flag to disable verification of the self-signed certificates: oscar-cli cluster add oscar-cluster https://localhost oscar <OSCAR_PASSWORD> --disable-ssl","title":"Using OSCAR-CLI"},{"location":"minio-bucket-replication/","text":"MinIO bucket replication In scenarios where you have two linked OSCAR clusters as part of the same workflow defined in FDL , temporary network disconnections cause that data generated on the first cluster during the disconnection time is lost as well. To resolve this scenario we propose the use of replicated buckets on MinIO. With this approach, you can have two buckets synchronized on different OSCAR clusters so that, if the connection is lost, they will be re-synchronized when the connection is restored. An example of this scenario is shown on the following diagram, where there are two MinIO instances (each one on a different OSCAR cluster), and the output of the execution of service_x on the source serves as input for the service_y on the remote cluster. Here is in more detail the data flow between the buckets: MinIO instance source input : receives data and triggers the execution of OSCAR service_x . intermediate : the output files from service_x are stored on this bucket and synchronized with the intermediate bucket on the remote instance. MinIO instance remote intermediate : the synchronized bucket that stores the replicated data and triggers OSCAR service_y . output : stores the output files of service_y . Considerations When you create the service on the remote OSCAR cluster, the intermediate bucket which is both the replica and input of the OSCAR service will have the webhook event for PUT actions enabled so it can trigger the OSCAR service. Because, as explained below on Event handling on replication events , there are some specific events for replicated buckets, it is important to delete this event webhook to avoid getting both events every time. mc event remove originminio/intermediate arn:aws:sqs::intermediate:webhook --event put Helm installation To be able to use replication each MinIO instance deployed with Helm has to be configured in distributed mode. This is done by adding the parameters mode=distributed,replicas=NUM_REPLICAS . Here is an example of a local MinIO replicated deployment with Helm: helm install minio minio/minio --namespace minio --set rootUser=minio,rootPassword=minio123,service.type=NodePort,service.nodePort=30300,consoleService.type=NodePort,consoleService.nodePort=30301,mode=distributed,replicas=2,resources.requests.memory=512Mi,environment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 --create-namespace MinIO setup To use the replication service it is necessary to set up manually both the requirements and the replication, either by command line or via the MinIO console. We created a test environment with replication via the command line as follows. First, we define our minIO instances ( originminio and remoteminio ) on the minio client. mc alias set originminio https://localminio minioadminuser minioadminpassword mc alias set remoteminio https://remoteminio minioadminuser minioadminpassword A requisite for replication is to enable the versioning on the buckets that will serve as origin and replica. When we create a service through OSCAR and the minIO buckets are created, versioning is not enabled by default, so we have to do it manually. mc version enable originminio/intermediate mc version enable remoteminio/intermediate Then, you can create the replication remote target mc admin bucket remote add originminio/intermediate \\ https://RemoteUser:Password@HOSTNAME/intermediate \\ --service \"replication\" and add the bucket replication rule so the actions on the origin bucket get synchronized on the replica. mc replicate add originminio/intermediate \\ --remote-bucket 'arn:minio:replication::<UUID>:intermediate' \\ --replicate \"delete,delete-marker,existing-objects\" Event handling on replication events Once you have replica instances you can add a specific event webhook for the replica-related events. mc event add originminio/intermediate arn:minio:sqs::intermediate:webhook --event replica The replication events sometimes arrive duplicated. Although this is not yet implemented, a solution to the duplicated events would be to filter them by the userMetadata , which is marked as \"PENDING\" on the events to be discarded. \"userMetadata\": { \"X-Amz-Replication-Status\": \"PENDING\" } MinIO documentation used Requirements to Set Up Bucket Replication Enable One-Way Server-Side Bucket Replication","title":"MinIO bucket replication"},{"location":"minio-bucket-replication/#minio-bucket-replication","text":"In scenarios where you have two linked OSCAR clusters as part of the same workflow defined in FDL , temporary network disconnections cause that data generated on the first cluster during the disconnection time is lost as well. To resolve this scenario we propose the use of replicated buckets on MinIO. With this approach, you can have two buckets synchronized on different OSCAR clusters so that, if the connection is lost, they will be re-synchronized when the connection is restored. An example of this scenario is shown on the following diagram, where there are two MinIO instances (each one on a different OSCAR cluster), and the output of the execution of service_x on the source serves as input for the service_y on the remote cluster. Here is in more detail the data flow between the buckets: MinIO instance source input : receives data and triggers the execution of OSCAR service_x . intermediate : the output files from service_x are stored on this bucket and synchronized with the intermediate bucket on the remote instance. MinIO instance remote intermediate : the synchronized bucket that stores the replicated data and triggers OSCAR service_y . output : stores the output files of service_y .","title":"MinIO bucket replication"},{"location":"minio-bucket-replication/#considerations","text":"When you create the service on the remote OSCAR cluster, the intermediate bucket which is both the replica and input of the OSCAR service will have the webhook event for PUT actions enabled so it can trigger the OSCAR service. Because, as explained below on Event handling on replication events , there are some specific events for replicated buckets, it is important to delete this event webhook to avoid getting both events every time. mc event remove originminio/intermediate arn:aws:sqs::intermediate:webhook --event put","title":"Considerations"},{"location":"minio-bucket-replication/#helm-installation","text":"To be able to use replication each MinIO instance deployed with Helm has to be configured in distributed mode. This is done by adding the parameters mode=distributed,replicas=NUM_REPLICAS . Here is an example of a local MinIO replicated deployment with Helm: helm install minio minio/minio --namespace minio --set rootUser=minio,rootPassword=minio123,service.type=NodePort,service.nodePort=30300,consoleService.type=NodePort,consoleService.nodePort=30301,mode=distributed,replicas=2,resources.requests.memory=512Mi,environment.MINIO_BROWSER_REDIRECT_URL=http://localhost:30301 --create-namespace","title":"Helm installation"},{"location":"minio-bucket-replication/#minio-setup","text":"To use the replication service it is necessary to set up manually both the requirements and the replication, either by command line or via the MinIO console. We created a test environment with replication via the command line as follows. First, we define our minIO instances ( originminio and remoteminio ) on the minio client. mc alias set originminio https://localminio minioadminuser minioadminpassword mc alias set remoteminio https://remoteminio minioadminuser minioadminpassword A requisite for replication is to enable the versioning on the buckets that will serve as origin and replica. When we create a service through OSCAR and the minIO buckets are created, versioning is not enabled by default, so we have to do it manually. mc version enable originminio/intermediate mc version enable remoteminio/intermediate Then, you can create the replication remote target mc admin bucket remote add originminio/intermediate \\ https://RemoteUser:Password@HOSTNAME/intermediate \\ --service \"replication\" and add the bucket replication rule so the actions on the origin bucket get synchronized on the replica. mc replicate add originminio/intermediate \\ --remote-bucket 'arn:minio:replication::<UUID>:intermediate' \\ --replicate \"delete,delete-marker,existing-objects\"","title":"MinIO setup"},{"location":"minio-bucket-replication/#event-handling-on-replication-events","text":"Once you have replica instances you can add a specific event webhook for the replica-related events. mc event add originminio/intermediate arn:minio:sqs::intermediate:webhook --event replica The replication events sometimes arrive duplicated. Although this is not yet implemented, a solution to the duplicated events would be to filter them by the userMetadata , which is marked as \"PENDING\" on the events to be discarded. \"userMetadata\": { \"X-Amz-Replication-Status\": \"PENDING\" } MinIO documentation used Requirements to Set Up Bucket Replication Enable One-Way Server-Side Bucket Replication","title":"Event handling on replication events"},{"location":"oidc-auth/","text":"OpenID Connect Authorization OSCAR REST API supports OIDC (OpenID Connect) access tokens to authorize users since release v2.5.0 . By default, OSCAR clusters deployed via the IM Dashboard are configured to allow authorization via basic auth and OIDC tokens using the EGI Check-in issuer. From the IM Dashboard deployment window, users can add one EGI Virtual Organization to grant access for all users from that VO. Accessing from OSCAR-UI The static web interface of OSCAR has been integrated with EGI Check-in and published in ui.oscar.grycap.net to facilitate the authorization of users. To login through EGI Check\u00edn using OIDC tokens, users only have to put the endpoint of its OSCAR cluster and click on the \"EGI CHECK-IN\" button. Integration with OSCAR-CLI via OIDC Agent Since version v1.4.0 OSCAR-CLI supports API authorization via OIDC tokens thanks to the integration with oidc-agent . Users must install the oidc-agent following its instructions and create a new account configuration for the https://aai.egi.eu/auth/realms/egi/ issuer. After that, clusters can be added with the command oscar-cli cluster add specifying the oidc-agent account name with the --oidc-account-name flag.","title":"OIDC Authorization"},{"location":"oidc-auth/#openid-connect-authorization","text":"OSCAR REST API supports OIDC (OpenID Connect) access tokens to authorize users since release v2.5.0 . By default, OSCAR clusters deployed via the IM Dashboard are configured to allow authorization via basic auth and OIDC tokens using the EGI Check-in issuer. From the IM Dashboard deployment window, users can add one EGI Virtual Organization to grant access for all users from that VO.","title":"OpenID Connect Authorization"},{"location":"oidc-auth/#accessing-from-oscar-ui","text":"The static web interface of OSCAR has been integrated with EGI Check-in and published in ui.oscar.grycap.net to facilitate the authorization of users. To login through EGI Check\u00edn using OIDC tokens, users only have to put the endpoint of its OSCAR cluster and click on the \"EGI CHECK-IN\" button.","title":"Accessing from OSCAR-UI"},{"location":"oidc-auth/#integration-with-oscar-cli-via-oidc-agent","text":"Since version v1.4.0 OSCAR-CLI supports API authorization via OIDC tokens thanks to the integration with oidc-agent . Users must install the oidc-agent following its instructions and create a new account configuration for the https://aai.egi.eu/auth/realms/egi/ issuer. After that, clusters can be added with the command oscar-cli cluster add specifying the oidc-agent account name with the --oidc-account-name flag.","title":"Integration with OSCAR-CLI via OIDC Agent"},{"location":"oscar-cli/","text":"OSCAR-CLI OSCAR-CLI provides a command line interface to interact with OSCAR clusters in a simple way. It supports service management, workflows definition from FDL (Functions Definition Language) files and the ability to manage files from OSCAR's compatible storage providers (MinIO, AWS S3 and Onedata). The folder example-workflow contains all the necessary files to create a simple workflow to test the tool. Download Releases The easy way to download OSCAR-CLI is through the github releases page . There are binaries for multiple platforms and OS. If you need a binary for another platform, please open an issue . Install from source If you have go installed and configured , you can get it from source directly by executing: go install github.com/grycap/oscar-cli@latest Available commands apply cluster add default info list remove service get list remove run logs list logs get logs remove get-file put-file list-files version help apply Apply a FDL file to create or edit services in clusters. Usage: oscar-cli apply FDL_FILE [flags] Aliases: apply, a Flags: --config string set the location of the config file (YAML or JSON) -h, --help help for apply cluster Manages the configuration of clusters. Subcommands add Add a new existing cluster to oscar-cli. Usage: oscar-cli cluster add IDENTIFIER ENDPOINT {USERNAME {PASSWORD | \\ --password-stdin} | --oidc-account-name ACCOUNT} [flags] Aliases: add, a Flags: --disable-ssl disable verification of ssl certificates for the added cluster -h, --help help for add -o, --oidc-account-name string OIDC account name to authenticate using oidc-agent. Note that oidc-agent must be started and properly configured (See:https://indigo-dc.gitbook.io/oidc-agent/) --password-stdin take the password from stdin Global Flags: --config string set the location of the config file (YAML or JSON) default Show or set the default cluster. Usage: oscar-cli cluster default [flags] Aliases: default, d Flags: -h, --help help for default -s, --set string set a default cluster by passing its IDENTIFIER Global Flags: --config string set the location of the config file (YAML or JSON) info Show information of an OSCAR cluster. Usage: oscar-cli cluster info [flags] Aliases: info, i Flags: -c, --cluster string set the cluster -h, --help help for info Global Flags: --config string set the location of the config file (YAML or JSON) list List the configured OSCAR clusters. Usage: oscar-cli cluster list [flags] Aliases: list, ls Flags: -h, --help help for list Global Flags: --config string set the location of the config file (YAML or JSON) remove Remove a cluster from the configuration file. Usage: oscar-cli cluster remove IDENTIFIER [flags] Aliases: remove, rm Flags: -h, --help help for remove Global Flags: --config string set the location of the config file (YAML or JSON) service Manages the services within a cluster. Subcommands of services get Get the definition of a service. Usage: oscar-cli service get SERVICE_NAME [flags] Aliases: get, g Flags: -c, --cluster string set the cluster -h, --help help for get Global Flags: --config string set the location of the config file (YAML or JSON) list services List the available services in a cluster. Usage: oscar-cli service list [flags] Aliases: list, ls Flags: -c, --cluster string set the cluster -h, --help help for list Global Flags: --config string set the location of the config file (YAML or JSON) remove services Remove a service from the cluster. Usage: oscar-cli service remove SERVICE_NAME... [flags] Aliases: remove, rm Flags: -c, --cluster string set the cluster -h, --help help for remove Global Flags: --config string set the location of the config file (YAML or JSON) run Invoke a service synchronously (a Serverless backend in the cluster is required). Usage: oscar-cli service run SERVICE_NAME {--input | --text-input} [flags] Aliases: run, invoke, r Flags: -c, --cluster string set the cluster -h, --help help for run -i, --input string input file for the request -o, --output string file path to store the output -t, --text-input string text input string for the request Global Flags: --config string set the location of the config file (YAML or JSON) logs list List the logs from a service. Usage: oscar-cli service logs list SERVICE_NAME [flags] Aliases: list, ls Flags: -h, --help help for list -s, --status strings filter by status (Pending, Running, Succeeded or Failed), multiple values can be specified by a comma-separated string Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON) logs get Get the logs from a service's job. Usage: oscar-cli service logs get SERVICE_NAME JOB_NAME [flags] Aliases: get, g Flags: -h, --help help for get -t, --show-timestamps show timestamps in the logs Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON) logs remove Remove a service's job along with its logs. Usage: oscar-cli service logs remove SERVICE_NAME \\ {JOB_NAME... | --succeeded | --all} [flags] Aliases: remove, rm Flags: -a, --all remove all logs from the service -h, --help help for remove -s, --succeeded remove succeeded logs from the service Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON) get-file Get a file from a service's storage provider. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service get-file SERVICE_NAME STORAGE_PROVIDER REMOTE_FILE \\ LOCAL_FILE [flags] Aliases: get-file, gf Flags: -c, --cluster string set the cluster -h, --help help for get-file Global Flags: --config string set the location of the config file (YAML or JSON) put-file Put a file in a service's storage provider. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service put-file SERVICE_NAME STORAGE_PROVIDER LOCAL_FILE \\ REMOTE_FILE [flags] Aliases: put-file, pf Flags: -c, --cluster string set the cluster -h, --help help for put-file Global Flags: --config string set the location of the config file (YAML or JSON) list-files List files from a service's storage provider path. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service list-files SERVICE_NAME STORAGE_PROVIDER REMOTE_PATH [flags] Aliases: list-files, list-file, lsf Flags: -c, --cluster string set the cluster -h, --help help for list-files Global Flags: --config string set the location of the config file (YAML or JSON) version Print the version. Usage: oscar-cli version [flags] Aliases: version, v Flags: -h, --help help for version help Help provides help for any command in the application. Simply type oscar-cli help [path to command] for full details. Usage: oscar-cli help [command] [flags] Flags: -h, --help help for help","title":"OSCAR-CLI"},{"location":"oscar-cli/#oscar-cli","text":"OSCAR-CLI provides a command line interface to interact with OSCAR clusters in a simple way. It supports service management, workflows definition from FDL (Functions Definition Language) files and the ability to manage files from OSCAR's compatible storage providers (MinIO, AWS S3 and Onedata). The folder example-workflow contains all the necessary files to create a simple workflow to test the tool.","title":"OSCAR-CLI"},{"location":"oscar-cli/#download","text":"","title":"Download"},{"location":"oscar-cli/#releases","text":"The easy way to download OSCAR-CLI is through the github releases page . There are binaries for multiple platforms and OS. If you need a binary for another platform, please open an issue .","title":"Releases"},{"location":"oscar-cli/#install-from-source","text":"If you have go installed and configured , you can get it from source directly by executing: go install github.com/grycap/oscar-cli@latest","title":"Install from source"},{"location":"oscar-cli/#available-commands","text":"apply cluster add default info list remove service get list remove run logs list logs get logs remove get-file put-file list-files version help","title":"Available commands"},{"location":"oscar-cli/#apply","text":"Apply a FDL file to create or edit services in clusters. Usage: oscar-cli apply FDL_FILE [flags] Aliases: apply, a Flags: --config string set the location of the config file (YAML or JSON) -h, --help help for apply","title":"apply"},{"location":"oscar-cli/#cluster","text":"Manages the configuration of clusters.","title":"cluster"},{"location":"oscar-cli/#subcommands","text":"","title":"Subcommands"},{"location":"oscar-cli/#add","text":"Add a new existing cluster to oscar-cli. Usage: oscar-cli cluster add IDENTIFIER ENDPOINT {USERNAME {PASSWORD | \\ --password-stdin} | --oidc-account-name ACCOUNT} [flags] Aliases: add, a Flags: --disable-ssl disable verification of ssl certificates for the added cluster -h, --help help for add -o, --oidc-account-name string OIDC account name to authenticate using oidc-agent. Note that oidc-agent must be started and properly configured (See:https://indigo-dc.gitbook.io/oidc-agent/) --password-stdin take the password from stdin Global Flags: --config string set the location of the config file (YAML or JSON)","title":"add"},{"location":"oscar-cli/#default","text":"Show or set the default cluster. Usage: oscar-cli cluster default [flags] Aliases: default, d Flags: -h, --help help for default -s, --set string set a default cluster by passing its IDENTIFIER Global Flags: --config string set the location of the config file (YAML or JSON)","title":"default"},{"location":"oscar-cli/#info","text":"Show information of an OSCAR cluster. Usage: oscar-cli cluster info [flags] Aliases: info, i Flags: -c, --cluster string set the cluster -h, --help help for info Global Flags: --config string set the location of the config file (YAML or JSON)","title":"info"},{"location":"oscar-cli/#list","text":"List the configured OSCAR clusters. Usage: oscar-cli cluster list [flags] Aliases: list, ls Flags: -h, --help help for list Global Flags: --config string set the location of the config file (YAML or JSON)","title":"list"},{"location":"oscar-cli/#remove","text":"Remove a cluster from the configuration file. Usage: oscar-cli cluster remove IDENTIFIER [flags] Aliases: remove, rm Flags: -h, --help help for remove Global Flags: --config string set the location of the config file (YAML or JSON)","title":"remove"},{"location":"oscar-cli/#service","text":"Manages the services within a cluster.","title":"service"},{"location":"oscar-cli/#subcommands-of-services","text":"","title":"Subcommands of services"},{"location":"oscar-cli/#get","text":"Get the definition of a service. Usage: oscar-cli service get SERVICE_NAME [flags] Aliases: get, g Flags: -c, --cluster string set the cluster -h, --help help for get Global Flags: --config string set the location of the config file (YAML or JSON)","title":"get"},{"location":"oscar-cli/#list-services","text":"List the available services in a cluster. Usage: oscar-cli service list [flags] Aliases: list, ls Flags: -c, --cluster string set the cluster -h, --help help for list Global Flags: --config string set the location of the config file (YAML or JSON)","title":"list services"},{"location":"oscar-cli/#remove-services","text":"Remove a service from the cluster. Usage: oscar-cli service remove SERVICE_NAME... [flags] Aliases: remove, rm Flags: -c, --cluster string set the cluster -h, --help help for remove Global Flags: --config string set the location of the config file (YAML or JSON)","title":"remove services"},{"location":"oscar-cli/#run","text":"Invoke a service synchronously (a Serverless backend in the cluster is required). Usage: oscar-cli service run SERVICE_NAME {--input | --text-input} [flags] Aliases: run, invoke, r Flags: -c, --cluster string set the cluster -h, --help help for run -i, --input string input file for the request -o, --output string file path to store the output -t, --text-input string text input string for the request Global Flags: --config string set the location of the config file (YAML or JSON)","title":"run"},{"location":"oscar-cli/#logs-list","text":"List the logs from a service. Usage: oscar-cli service logs list SERVICE_NAME [flags] Aliases: list, ls Flags: -h, --help help for list -s, --status strings filter by status (Pending, Running, Succeeded or Failed), multiple values can be specified by a comma-separated string Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON)","title":"logs list"},{"location":"oscar-cli/#logs-get","text":"Get the logs from a service's job. Usage: oscar-cli service logs get SERVICE_NAME JOB_NAME [flags] Aliases: get, g Flags: -h, --help help for get -t, --show-timestamps show timestamps in the logs Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON)","title":"logs get"},{"location":"oscar-cli/#logs-remove","text":"Remove a service's job along with its logs. Usage: oscar-cli service logs remove SERVICE_NAME \\ {JOB_NAME... | --succeeded | --all} [flags] Aliases: remove, rm Flags: -a, --all remove all logs from the service -h, --help help for remove -s, --succeeded remove succeeded logs from the service Global Flags: -c, --cluster string set the cluster --config string set the location of the config file (YAML or JSON)","title":"logs remove"},{"location":"oscar-cli/#get-file","text":"Get a file from a service's storage provider. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service get-file SERVICE_NAME STORAGE_PROVIDER REMOTE_FILE \\ LOCAL_FILE [flags] Aliases: get-file, gf Flags: -c, --cluster string set the cluster -h, --help help for get-file Global Flags: --config string set the location of the config file (YAML or JSON)","title":"get-file"},{"location":"oscar-cli/#put-file","text":"Put a file in a service's storage provider. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service put-file SERVICE_NAME STORAGE_PROVIDER LOCAL_FILE \\ REMOTE_FILE [flags] Aliases: put-file, pf Flags: -c, --cluster string set the cluster -h, --help help for put-file Global Flags: --config string set the location of the config file (YAML or JSON)","title":"put-file"},{"location":"oscar-cli/#list-files","text":"List files from a service's storage provider path. The STORAGE_PROVIDER argument follows the format STORAGE_PROVIDER_TYPE.STORAGE_PROVIDER_NAME, being the STORAGE_PROVIDER_TYPE one of the three supported storage providers (MinIO, S3 or Onedata) and the STORAGE_PROVIDER_NAME is the identifier for the provider set in the service's definition. Usage: oscar-cli service list-files SERVICE_NAME STORAGE_PROVIDER REMOTE_PATH [flags] Aliases: list-files, list-file, lsf Flags: -c, --cluster string set the cluster -h, --help help for list-files Global Flags: --config string set the location of the config file (YAML or JSON)","title":"list-files"},{"location":"oscar-cli/#version","text":"Print the version. Usage: oscar-cli version [flags] Aliases: version, v Flags: -h, --help help for version","title":"version"},{"location":"oscar-cli/#help","text":"Help provides help for any command in the application. Simply type oscar-cli help [path to command] for full details. Usage: oscar-cli help [command] [flags] Flags: -h, --help help for help","title":"help"},{"location":"usage/","text":"Using OSCAR through the web-based UI OSCAR allows the creation of serverless file-processing services based on container images. These services require a user-defined script with the commands responsible of the processing. The platform automatically mounts a volume on the containers with the FaaS Supervisor component, which is in charge of: Downloading the file that invokes the service and make it accessible through the INPUT_FILE_PATH environment variable. Execute the user-defined script. Upload the content of the output folder accessible via the TMP_OUTPUT_DIR environment variable. You can follow one of the examples in order to test the OSCAR framework for specific applications. We recommend you to start with the plant classification example detailed below. If you prefer to use the command-line interface rather than the web-based UI, there is an example in oscar-cli's repository . Login OSCAR is exposed via a Kubernetes ingress and it is accessible via the Kubernetes master node IP. If you deployed it using EC3 you can find the credentials here . After a correct login, you should see the main view: Deploying services In order to create a new service, you must click on the \"DEPLOY NEW SERVICE\" button and follow the wizard. Remember that a script must be provided for the processing of files. This script must use the environment variables INPUT_FILE_PATH and TMP_OUTPUT_DIR to refer to the input file and the folder where to save the results respectively: #!/bin/bash echo \"SCRIPT: Invoked classify_image.py. File available in $INPUT_FILE_PATH\" FILE_NAME=`basename \"$INPUT_FILE_PATH\"` OUTPUT_FILE=\"$TMP_OUTPUT_DIR/$FILE_NAME\" python2 /opt/plant-classification-theano/classify_image.py \\ \"$INPUT_FILE_PATH\" -o \"$OUTPUT_FILE\" You must fill in the fields indicating the container image to use, the name of the service and the script file. In addition, you can add environment variables, specify the resources (RAM and CPUs) and choose the log level of the service. Note that specifying a tag in the container image used can be convenient to avoid problems with quotas for certain container registries such as Docker Hub . This is due to the fact that Kubernetes defaults the imagePullPolicy of pods to Always when no tag or the latest tag is set, which checks the version of the image in the registry every time a job is launched. Next, the credentials of the storage providers to be used must be introduced. As the platform already has a MinIO deployment to operate, it is not necessary to enter its credentials for using it. Multiple MinIO, Onedata and Amazon S3 storage providers can be used. Remember to click the \"ADD\" button after completing each one. Then, click the \"NEXT\" button to go to the last section of the wizard. In this section, you must first choose the paths of the storage provider to be used as source of events, i.e. the input bucket and/or folder that will trigger the service. Only the minio.default provider can be used as input storage provider. After filling in each path, remember to click on the \"ADD INPUT\" button. Finally, the same must be done to indicate the output paths to be used in the desired storage providers. You can also indicate suffixes and/or prefixes to filter the files uploaded to each path by name. The resulting files can be stored in several storage providers, like in the following example, where they are stored in the MinIO server of the platform and in a Onedata space provided by the user. After clicking the \"SUBMIT\" button the new service will appear in the main view after a few seconds. Triggering the service HTTP endpoints OSCAR services can be invoked through auto-generated HTTP endpoints. Requests to these endpoints can be made in two ways: Synchronous through the path /run/<SERVICE_NAME> . This redirects the request to the OpenFaaS gateway in order to perform the processing. Asynchronous through the path /job/<SERVICE_NAME> . This mode is used to perform file-processing when files are uploaded to the input storage provider, creating a Kubernetes job per service invocation. The content of the HTTP request body will be stored as a file that will be available via the INPUT_FILE_PATH environment variable to process it. A detailed specification of the OSCAR's API and its different paths can be found here . MinIO Storage Tab MinIO Storage Tab is made to manage buckets without using MinIO UI. It simplifies the process. From MinIO Storage Tab, buckets can be created or removed and folders inside them. Furthermore, files can be uploaded to the buckets and downloaded from them. Each time a service is created or submitted an edit, the buckets that are not created will be formed. Uploading files Once a service has been created, it can be invoked by uploading files to its input bucket/folder. This can be done through the MinIO web interface (accessible from the Kubernetes frontend IP, on port 30300 ) or from the \"Minio Storage\" section in the side menu of the OSCAR web interface. Expanding down that menu will list the buckets created and, by clicking on their name, you will be able to see their content, upload and download files. To upload files, first click on the \"SELECT FILES\" button and choose the files you want to upload from your computer. Once you have chosen the files to upload, simply click on the \"UPLOAD\" button and the file will be uploaded, raising an event that will trigger the service. Note that the web interface includes a preview button for some file formats, such as images. Service status and logs When files are being processed by a service, it is important to know their status, as well as to observe the execution logs for testing. For this purpose, OSCAR includes a log view, accessible by clicking on the \"LOGS\" button in a service from the main view. In this view you can see all the jobs created for a service, as well as their status (\"Pending\", \"Running\", \"Succeeded\" or \"Failed\") and their creation, start and finish time. To view the logs generated by a job, simply click on the drop-down button located on the right. The view also features options to refresh the status of one or all jobs, as well as to delete them. Downloading files from MinIO Downloading files from the platform's MinIO storage provider can also be done using the OSCAR web interface. To do it, simply select one or more files and click on the button \"DOWNLOAD OBJECT\" (or \"DOWNLOAD ALL AS A ZIP\" if several files have been selected). In the following picture you can see the preview of the resulting file after the execution triggered in the previous step. Deleting services Services can be deleted by clicking on the trash can icon from the main view. Once you have accepted the message shown in the image above, the service will be deleted after a few seconds.","title":"Using OSCAR through the web-based UI"},{"location":"usage/#using-oscar-through-the-web-based-ui","text":"OSCAR allows the creation of serverless file-processing services based on container images. These services require a user-defined script with the commands responsible of the processing. The platform automatically mounts a volume on the containers with the FaaS Supervisor component, which is in charge of: Downloading the file that invokes the service and make it accessible through the INPUT_FILE_PATH environment variable. Execute the user-defined script. Upload the content of the output folder accessible via the TMP_OUTPUT_DIR environment variable. You can follow one of the examples in order to test the OSCAR framework for specific applications. We recommend you to start with the plant classification example detailed below. If you prefer to use the command-line interface rather than the web-based UI, there is an example in oscar-cli's repository .","title":"Using OSCAR through the web-based UI"},{"location":"usage/#login","text":"OSCAR is exposed via a Kubernetes ingress and it is accessible via the Kubernetes master node IP. If you deployed it using EC3 you can find the credentials here . After a correct login, you should see the main view:","title":"Login"},{"location":"usage/#deploying-services","text":"In order to create a new service, you must click on the \"DEPLOY NEW SERVICE\" button and follow the wizard. Remember that a script must be provided for the processing of files. This script must use the environment variables INPUT_FILE_PATH and TMP_OUTPUT_DIR to refer to the input file and the folder where to save the results respectively: #!/bin/bash echo \"SCRIPT: Invoked classify_image.py. File available in $INPUT_FILE_PATH\" FILE_NAME=`basename \"$INPUT_FILE_PATH\"` OUTPUT_FILE=\"$TMP_OUTPUT_DIR/$FILE_NAME\" python2 /opt/plant-classification-theano/classify_image.py \\ \"$INPUT_FILE_PATH\" -o \"$OUTPUT_FILE\" You must fill in the fields indicating the container image to use, the name of the service and the script file. In addition, you can add environment variables, specify the resources (RAM and CPUs) and choose the log level of the service. Note that specifying a tag in the container image used can be convenient to avoid problems with quotas for certain container registries such as Docker Hub . This is due to the fact that Kubernetes defaults the imagePullPolicy of pods to Always when no tag or the latest tag is set, which checks the version of the image in the registry every time a job is launched. Next, the credentials of the storage providers to be used must be introduced. As the platform already has a MinIO deployment to operate, it is not necessary to enter its credentials for using it. Multiple MinIO, Onedata and Amazon S3 storage providers can be used. Remember to click the \"ADD\" button after completing each one. Then, click the \"NEXT\" button to go to the last section of the wizard. In this section, you must first choose the paths of the storage provider to be used as source of events, i.e. the input bucket and/or folder that will trigger the service. Only the minio.default provider can be used as input storage provider. After filling in each path, remember to click on the \"ADD INPUT\" button. Finally, the same must be done to indicate the output paths to be used in the desired storage providers. You can also indicate suffixes and/or prefixes to filter the files uploaded to each path by name. The resulting files can be stored in several storage providers, like in the following example, where they are stored in the MinIO server of the platform and in a Onedata space provided by the user. After clicking the \"SUBMIT\" button the new service will appear in the main view after a few seconds.","title":"Deploying services"},{"location":"usage/#triggering-the-service","text":"","title":"Triggering the service"},{"location":"usage/#http-endpoints","text":"OSCAR services can be invoked through auto-generated HTTP endpoints. Requests to these endpoints can be made in two ways: Synchronous through the path /run/<SERVICE_NAME> . This redirects the request to the OpenFaaS gateway in order to perform the processing. Asynchronous through the path /job/<SERVICE_NAME> . This mode is used to perform file-processing when files are uploaded to the input storage provider, creating a Kubernetes job per service invocation. The content of the HTTP request body will be stored as a file that will be available via the INPUT_FILE_PATH environment variable to process it. A detailed specification of the OSCAR's API and its different paths can be found here .","title":"HTTP endpoints"},{"location":"usage/#minio-storage-tab","text":"MinIO Storage Tab is made to manage buckets without using MinIO UI. It simplifies the process. From MinIO Storage Tab, buckets can be created or removed and folders inside them. Furthermore, files can be uploaded to the buckets and downloaded from them. Each time a service is created or submitted an edit, the buckets that are not created will be formed.","title":"MinIO Storage Tab"},{"location":"usage/#uploading-files","text":"Once a service has been created, it can be invoked by uploading files to its input bucket/folder. This can be done through the MinIO web interface (accessible from the Kubernetes frontend IP, on port 30300 ) or from the \"Minio Storage\" section in the side menu of the OSCAR web interface. Expanding down that menu will list the buckets created and, by clicking on their name, you will be able to see their content, upload and download files. To upload files, first click on the \"SELECT FILES\" button and choose the files you want to upload from your computer. Once you have chosen the files to upload, simply click on the \"UPLOAD\" button and the file will be uploaded, raising an event that will trigger the service. Note that the web interface includes a preview button for some file formats, such as images.","title":"Uploading files"},{"location":"usage/#service-status-and-logs","text":"When files are being processed by a service, it is important to know their status, as well as to observe the execution logs for testing. For this purpose, OSCAR includes a log view, accessible by clicking on the \"LOGS\" button in a service from the main view. In this view you can see all the jobs created for a service, as well as their status (\"Pending\", \"Running\", \"Succeeded\" or \"Failed\") and their creation, start and finish time. To view the logs generated by a job, simply click on the drop-down button located on the right. The view also features options to refresh the status of one or all jobs, as well as to delete them.","title":"Service status and logs"},{"location":"usage/#downloading-files-from-minio","text":"Downloading files from the platform's MinIO storage provider can also be done using the OSCAR web interface. To do it, simply select one or more files and click on the button \"DOWNLOAD OBJECT\" (or \"DOWNLOAD ALL AS A ZIP\" if several files have been selected). In the following picture you can see the preview of the resulting file after the execution triggered in the previous step.","title":"Downloading files from MinIO"},{"location":"usage/#deleting-services","text":"Services can be deleted by clicking on the trash can icon from the main view. Once you have accepted the message shown in the image above, the service will be deleted after a few seconds.","title":"Deleting services"}]}